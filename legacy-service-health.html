<html lang="en">
  <head>
    <meta property="og:title" content="César González Segura - Article title">
    <meta property="og:description" content="Article description">
    <meta property="og:image" content="https://cesargonzalez.dev/photo.webp">
    <meta property="og:url" content="https://cesargonzalez.dev">
    <meta name="description" content="César González Segura Personal - Website">
    <meta charset="utf-8"/>
    <title>César González</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="./article.css">
  </head>
  <body>
    <header>
      <img src="images/photo.webp" alt="My Picture" />
      <h1>César González Segura</h1>
    </header>

    <div class="contact">
      <a aria-label="Telegram Contact" href="https://t.me/cegonse"><img src="images/telegram.webp" alt="Telegram Logo"></a>
      <a aria-label="LinkedIn Profile" href="https://www.linkedin.com/in/cesar-gonzalez-segura/"><img src="images/linkedin.webp" alt="LinkedIn Logo"></a>
      <a aria-label="Email Contact" href="mailto:contact@cesargonzalez.dev"><img src="images/email.webp" alt="Email Symbol"></a>
      <a aria-label="GitHub Profile" href="https://github.com/cegonse"><img src="images/github.webp" alt="GitHub Logo"></a>
    </div>

    <h2>A workshop to help your team navigate legacy software</h2>
    <p class="italic">Posted February 16th 2026</p>
    <p class="italic">Did you enjoy this article? Follow the conversation on <a>LinkedIn</a></p>

    <article>
      <h2>The importance of properly maintaining legacy software</h2>
      <p>
        If you have been around software engineering communities for a while, you will know that dealing with legacy
        software is not the preference for most people in our industry.
      </p>
      <p>
        Working with legacy software usually gets a bad rap. Many engineers associate the word <span class="italic">legacy</span>
        with incomprehensible codebases, a tangled mess of contributions built over decades.
      </p>
      <p>
        The reality is that in many cases, legacy software is what keeps businesses afloat. Forgotten services, silently processing
        millions of transactions year over year, generating revenue to support their operations.
      </p>
      <p>
        These isn't the kind of software that attracts most engineers, looking for the newest shiny thing in the scene.
        But this software is what keeps our modern world turning.
      </p>
      <p>
        As time passes, software starts decaying. Maybe the original team who bootstraped the project isn't around anymore,
        and institutional knowledge has been lost.
      </p>
      <p>
        Or maybe its based on ancient (for software engineering standards, anything older than a few years) technology, and nobody
        around knows how to work with it anymore.
      </p>
      <p>
        At the same time, there's little incentive from a business perspective to take legacy software to modern standards. If it keeps working,
        why waste resources on it? There's always some new product, some new feature that will have higher priority.
      </p>
      <p>
        However, the world doesn't stand still and is continuously evolving. And the software backing our modern world has to adapt
        to these changes, sometimes very abruptly.
      </p>
      <p>
        What happens when your organization's legacy software is in a state that can't evolve as required? Business continuity gets in danger.
      </p>
      <p>
        From paradigm shifts like the desktop to mobile transition, the adoption of cloud native infrastructure or abrupt regulatory changes
        as we're experiencing during 2025 and 2026, a lot of things can happen that force you to adapt your legacy software quickly.
      </p>
      <p>
        In one of my previous teams, we were in a position like this. We were in charge of a sizeable number of services and products,
        suffering from all the common ills of legacy software.
      </p>
      <p>
        We had the task of transforming it all, but with limited time and resources, we wouldn't be able to do it all on time. So the question was:
        what should we do first? Why this piece of software and not another one?
      </p>
      <p>
        As a way to help us get out of the deadlock we were in, I worked with my team on building this workshop that helped us define what has to be fixed,
        and what should we do first to ensure we will be able to adapt on time to the business needs.
      </p>
    </article>

    <article>
      <h2>Introduction to the workshop</h2>
      <p>
        The objective of this workshop is helping your team:
        <ul>
          <li>Understand what are the most pressing issues with your legacy services.</li>
          <li>Plan what fixes should be done first to ensure business continuity.</li>
          <li>Share knowledge about legacy systems with the whole team.</li>
        </ul>
      </p>
      <p>
        The workshop consists of <b>two sessions</b>, taking around <b>two hours</b> each. It's important
        that all your team is able to assist to the workshop.
      </p>
      <p>
        Preparation work for both sessions can take around other <b>2/3 hours</b> before each session. You can do
        the prep work by yourself with help from other team member.
      </p>
      <p>
        To get the most out of the workshop, it's best to limit the number of attendees to under <b>10 people</b>.
        You want to ensure everybody has enough time and space to contribute.
      </p>
      <p>
        The purpose of the two sessions are:
        <ul>
          <li><b>Mapping service health</b>
            <ul>
              <li>Understanding the technical health of the services maintained by your team.</li>
              <li>Outcome &rarr; Know in what shape are your services.</li>
            </ul>
          </li>
          <li><b>Mapping business / service relevance</b>
            <ul>
              <li>Understand the business relevance of your team's services, and detect pinpoint problems.</li>
              <li>Outcome &rarr; Know what services are worth fixing from a business point of view.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        By the end of the workshop, you should have a picture of what product features are under threat due to problems
        in the legacy services backing them, and which should be solved first.
      </p>
    </article>

    <article>
      <h2>Setting up an example case study</h2>
      <p>
        To illustrate how could the workshop be run with a concrete example, let's introduce an imaginary product and its stack.
      </p>
      <p>
        Let's assume we have an e-commerce platform, backed by several in-house services and other off-the-shelf or external SaaS
        services.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/case-study.png" />
        <span>Service stack in our platform.</span>
      </div>
      <p>
        In this workshop, we will only focus on our <b>in-house services</b>, ignoring off-the-shelf and external parts of the
        system.
      </p>
    </article>

    <article>
      <h2>Session #1 - Mapping service health</h2>
      <p>
        This workshop session is designed to help the team reflect about the services they take care of,
        and evaluate their health. A 2~3 hour slot should suffice for this session, but you will have to
        prepare the workshop materials beforehand.
      </p>
      <p>
        The team will have to describe their services through <b>metrics</b>, helping them measure their
        health. These metrics can be both <b>quantitative</b> or <b>qualitative</b>. Depending on your context,
        some metrics will make more sense than others.
      </p>
      <p>
        I recommend preparing the workshop with someone who has been working with these systems for a long time.
        They will be able to help you define metrics that the team will find easy to relate to when characterizing
        their services.
      </p>
      <p>
        These metrics carry an associated scoring, mapping their health from best case to worst case. In our workshop,
        we used a simple scoring system ranging 1~4, together with a simple semaphoric color code to help us map metrics
        in a digital whiteboard:
        <ul>
          <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>) - Ideal</b> &rarr; As good as it can be. Our north-star regarding how we want our services to be.</li>
          <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>) - Acceptable</b> &rarr; Rough around its edges, but perfectly manageable for a production service.</li>
          <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) - Problematic</b> &rarr; Problems are becoming worrying, and could get worse if maintenance is not expedited.</li>
          <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>) - All hope is lost</b> &rarr; A ticking time bomb, we are not ready to react if something unexpected happens.</li>
        </ul>
      </p>
      <p>
        Once these metrics are ready, we'll set-up a service / health matrix in our digital whiteboard to let the team discuss and
        assign a score to each one of our services.
      </p>
      <p>
        Start by creating a matrix with all the services, and one by one, let the team
        decide which score makes more sense for each one of the metrics:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/empty-matrix.png" />
        <span>Start with all your services, and the metrics you have chosen.</span>
      </div>
      <p>
        Then, start a discussion for each of the services and start assigning a score to each metric. As we'll see in the next section,
        some metrics will be objectively measurable, while others will raise a more heated debate.
      </p>
      <p>
        This is by design: we want our team to have a healthy discussion about the services we are managing, and give ourselves some time
        to reflect about our work.
      </p>
      <p>
        Assign a score from 1~4 (green to black, or the color palette of your liking), and finally assign the aggregated score for each of
        them. A simple scoring formula that worked well is <span class="inline-code">ceil(avg([scores]))</span>.
      </p>
      <p>
        This formula pulls scores down, ensuring that scores of services with low ranking metrics get pulled low. This helps to bring attention
        to services that need it.
      </p>
      <p>
        Let's add the scores for the metrics, and calculate the score for each metric:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/filled-matrix.png" />
        <span>All services have been evaluated and assigned a score.</span>
      </div>
      <p>
        With these results, we have a bird's-eye view of the technical state of our teams services, split by each metric. With this framework
        in mind, we will have to define which metrics can be used to measure the health our system.
      </p>
      <p>
        In the next section, you'll find some example metrics that you can use in your own workshop. And later on,
        we'll review the example case study with a set of metrics that could be good candidates for the workshop.
      </p>
    </article>
    <article>
      <h2>Quantitative metrics</h2>
      <p>
        Quantitative metrics are those that can be measured from objective data. These can be
        useful to determine how far are your team's services from the goals set by your organization,
        upcoming regulatory requirements, etc.
      </p>
      <p>
        For each metric, you should define which are the measurable points and/or thresholds to set
        the metric's score. For example, consider a metric with four yes/no questions, where "yes" is
        positive (you want to fulfill all items).
      </p>
      <p>
        In this case, having a "yes" answer to all points would score a 1 (<span style="color: rgb(196, 246, 123)">▣</span>),
        having a "yes" to three out of four would score a 2 (<span style="color: rgb(252, 164, 0)">▣</span>), and so on.
      </p>
      <p>
        Here are some examples you can use as a starting point. Some of these metrics can be answered on a
        yes/no basis, others require gathering actual numbers:
      </p>
      <ul>
        <li>
          <b>Obsevability</b>
            <ul>
              <li>Are our systems observable from the outside?</li>
              <li>Are there alarms in place to notify the team in case there is a production failure?</li>
              <li>Is the team able to understand the current state of the system by checking logs and traces?</li>
              <li>Are performance measurements available for our systems?</li>
            </ul>
        </li>
        <li>
          <b>Security</b>
          <ul>
            <li>Do our systems use libraries or frameworks with known security vulnerabilities?</li>
            <li>Are we running security scanning processes on our systems?</li>
            <li>If so, is the team actively patching found security vulnerabilities?
            <li>Does the team follow security best practices? (masking PII data in logs, properly storing sensitive data and secrets, etc)</li>
          </ul>
        </li>
        <li>
          <b>Up-to-dateness</b>:
          <ul>
            <li>Are any of our systems using frameworks or libraries out of their maintenance window?</li>
            <li>Are any systems using frameworks that could complicate updating to modern versions if needed? (ex. NET Framework, PHP 5...)</li>
            <li>Do our systems comply with all applicable regulations, in all applicable jurisdictions?</li>
          </ul>
        </li>
        <li>
          <b>Scalability</b>
            <ul>
              <li>Is the team aware of how do our services perform under load?</li>
              <li>Does the architecture of the systems allow to scale them up when required?</li>
              <li>Can the team scale them autonomously, or do we need support or permission from other teams?</li>
              <li>Are there auto-scaling measures in place?</li>
              <li>Do we perform regular load testing on our services?</li>
          </ul>
        </li>
        <li>
          <b>Failure Rate</b>
          <ul>
            <li>How often does the service suffer from critical bugs?</li>
            <li>Does the service go down? If so, how long does its recovery take?</li>
            <li>Is the service part of the critical path? Can the business continue operating during downtime?</li>
          </ul>
        </li>
      </ul>
    </article>
    <article>
      <h2>Qualitative metrics</h2>
      <p>
        Qualitative metrics work great to help your team assess how do they "feel" when working with
        their systems. Discussing metrics without factual data to back them might seem counterproductive,
        but in some cases trying to express all problems with hard numbers might complicate the debate.
      </p>
      <p>
        The main point when discussing through these metrics is ensuring there's a healthy debate during the
        workshop, and come to a common understanding of how does it <i>feel</i> to work with these systems.
      </p>
      <p>
        Some qualitative metrics you can use are:
      </p>
      <ul>
        <li>
          <b>Extendability</b>
          <ul>
            <li>How easy is it to add new features to our service?</li>
            <li>Are we held back by its architecture and/or design?</li>
            <li>Do we avoid extending the service beyond surgical changes due to accumulated tech debt?</li>
          </ul>
        </li>
        <li>
          <b>Testing quality</b>
          <ul>
            <li>Are the tests backing our service easy to read and understand?</li>
            <li>Do we trust the results of our tests?</li>
            <li>Is it easy to create new tests?</li>
            <li>Does our test codebase break easily when adding new features?</li>
          </ul>
        </li>
      </ul>
      <p>
        We could find an objective way of measuring these metrics with hard data. But in many
        cases the debate created around these topics is much more valuable to understand the general
        "vibe" around working with these services, than hard numbers by themselves.
      </p>
      <p>
        For example, you could assess testing quality quantitatively by measuring the code base's coverage,
        the number of flaky tests or the time required to execute a full test suite.
      </p>
      <p>
        However, if the architecture supporting these tests makes creating new tests or extending existing
        tests a chore, the team will have an easier time expressing this pain point from a feelings
        point of view. Even though from a numbers perspective, everything seems to be okay.
      </p>
    </article>

    <article>
      <h2>Session #1 - example case study</h2>
      <p>
        Let's go back to our example case study, and define which metrics are most relevant for our services, and how
        to measure them.
      </p>
      <p>
        Our service matrix will consist of the four key services our team manages (excluding off-the-shelf
        components). To measure their health, we will use four metrics, two qualitative and two quantitative:
        <ul>
          <li>
            <b>Extendability</b>: How easy is it to extend the service with new features?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Extending the service with new functionality is seamless. New features and improvements take minimal time.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; Adding new functionalities can get cumbersome without refactoring first.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; The codebase has grown without regard to its architecture, and adding new features is time consuming and a potential risk.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; Adding new features is very time consuming, and requires extensive prior clean up through refactoring.</li>
            </ul>
          </li>
          <li>
            <b>Testing Quality</b>: How good are the tests supporting development of the service?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Tests are well maintained, comprehensive and robust. Includes functional and non-functional testing, stress tests, etc...</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; There is decent test coverage, but their design makes them prone to breaking.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; There are some tests, but not coverage isn't good enough to allow changes without risk.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; There are no tests, and adding new tests would take a lot of effort at this point.</li>
            </ul>
          </li>
          <li>
            <b>Failure Rate</b>: How often does the system fail? How long does it take to recover?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Less than 5 critical failures in a one year period. Upon failure, full recovery took less than 10 minutes.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; ~10 critical failures YoY. When these happened, full recovery took less than 10 minutes.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; ~10 critical failures YoY. When these happened, full recovery took more than 10 minutes.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; ~4 critical failures / week. When these happened, full recovery took more than 10 minutes.</li>
            </ul>
          </li>
          <li>
            <b>Security & Up-to-Date</b>: Are its dependencies (including frameworks) up to date? Have security vulnerabilities been detected?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Zero critical security vulnerabilities. Dependencies use latest stable versions.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; Zero critical security vulnerabilities. Dependencies use older but still supported stable versions.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; Active critical security vulnerabilities present. Dependencies use older but still supported stable versions.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; Active critical security vulnerabilities present. Dependencies use deprecated versions.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        Now that we have a set of metrics to measure the health of our teams services, we can build the service / metric health matrix
        and start filling in the score for each metric.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/case-study-empty-matrix.png" />
        <span>Empty service health map.</span>
      </div>
      <p>
        If we have been diligent keeping track of our work in the available tracking tools, deciding the score for the quantitative metrics
        should be straightforward.
      </p>
      <p>
        We can leverage logs, performance metrics, reports from the package manager, security vulnerability analysis tools and post-mortem documentation
        from previous failures and incidents.
      </p>
      <p>
        On the other hand, extendability and testing quality metrics will involve a deeper discussion between the developers in the team. How was the
        experience building the latest features in each service? Was it easy, or did we try to avoid it at all costs?
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/case-study-filled-matrix.png" />
        <span>Completed service health map (numerical score added for clarity).</span>
      </div>
      <p>
        After some healthy discussion in the team, metrics for all services have been evaluated and their scores set. The final score has been calculated
        following the formula introduced earlier.
      </p>
      <p>
        Now, we have a birds-eye level picture of the health of our services. From what we've gathered, our <b>web-app</b> and <b>search service</b> are in a good
        enough shape, at an acceptable point for a production app.
      </p>
      <p>
        However, the <b>billing</b> and <b>ratings</b> service are in trouble. Particularly the <b>ratings service</b> has strong ongoing issues, with a very high failure rate, lurking
        security issues and no useful test base to help us overcome these problems.
      </p>
      <p>
        Should we jump in and plan the work to get these services into shape into next iterations? First, we should ask ourselves: <b>is it worth it?</b>
      </p>
    </article>

    <article>
      <h2>Session #2 prep - Considering bussiness importance</h2>
      <p>
        If we only pursued engineering excellence, and were provided with unlimited budget, it would be great to fix all the problems we've found,
        and leave them in perfect condition.
      </p>
      <p>
        In real world software engineering, with limited time, capacity and rapidly switching objectives, we have to compromise.
      </p>
      <p>
        Before even starting to think about how to fix the issues we've found, first we had to understand if it made sense from a business perspective.
      </p>
      <p>
        To do so, we scheduled a short follow-up session with product and business specialists from our team, and listed the product features that are
        backed by our team's services.
      </p>
      <p>
        Then, we sorted all features by <b>business importance</b>, using a numerical scale as we did with the technical services.
      </p>
      <p>
        Ranging from 1 (<span style="color: rgb(196, 246, 123)">▣</span>), meaning a feature that's very important for the business, to
        3 (<span style="color: rgb(255, 88, 88)">▣</span>), meaning it's a nice to have but not really a must have for business continuity.
      </p>
      <p>
        To support this classification, we used a variety of metrics our product experts had in hand. Some of them were:
        <ul>
          <li><b>Usage numbers</b>
            <ul>
              <li>How many users do actively use the feature? (DAU / MAU)</li>
              <li>Are the users making use of the feature important for the business? (paying customers, important contracts...)</li>
            </ul>
          </li>
          <li><b>Revenue Generation</b>
            <ul>
              <li>What is the impact of the feature in terms of revenue, regardless of usage?</li>
              <li>What would happen to revenue if we stopped supporting it?</li>
            </ul>
          </li>
          <li><b>Compliance & Strategical Opportunity</b>
            <ul>
              <li>Is this feature a regulatory requirement in important markets?</li>
              <li>Does supporting it keep key users from leaving the platform?</li>
              <li>Is it a requirement for an important contract with a major customer?</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        With the list of features in hand, sorted by their business relevance, we can prepare the next workshop session: matching each
        product feature with its backing service.
      </p>
    </article>

    <article>
      <h2>Session #2 - Evaluating technical / business health</h2>
      <p>
        It's time for the second and final session of the workshop. It will take around 1~2 hours to complete.
      </p>
      <p>
        In this session, we will complete link the business relevance of each product feature to services supporting them.
      </p>
      <p>
        To build this mapping, the steps we follow are:
        <ol>
          <li>Build a matrix, with all services as rows, and product features as columns.</li>
          <li>
            Sort both services and features with <b>descending health and relevance</b> (top left corner for best, bottom right corner for worst).
            <ul><li>Use the color key to help visualize health and relevance at a glance.</li></ul>
          </li>
          <li>
            Place a <b>pin</b> in each cell where a product feature is backed by a service.
            <ul><li>In the following example, Feature A is backed by Services 1 and 2.</li></ul>
          </li>
        </ol>
      </p>
      <p>
        The resulting matrix will look like this:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-matrix.png" />
        <span>Mapping product features against services.</span>
      </div>
      <p>
        With this information at our disposal, its time to start taking decisions on what's worth fixing and what's not. We'll focus
        on looking for features that are:
        <ul>
          <li>In good shape.</li>
          <li>Ticking time bombs.</li>
          <li>Costing the business more money than they're worth it.</li>
        </ul>
      </p>
      <p>
        First, we have the best case scenario: features that are very relevant for the business, which are supported by
        services that are in good shape.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-all-good.png" />
        <span>No need to worry about these.</span>
      </div>
      <p>
        This is great! These features are the ones that make our product relevant, and the services backing them are
        in good condition.
      </p>
      <p>
        We can safely ignore these, and just keep up the good work. If there are any new features, unexpected regulatory changes
        or required updates, we will be able to handle them without problems.
      </p>
      <p>
        Next, we must focus on the <b>danger zone</b>. These services are a ticking bomb in our hands, and we should be looking
        forward to fixing the issues we've detected as soon as possible.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-danger-zone.png" />
        <span>We could be in trouble if something goes wrong with these.</span>
      </div>
      <p>
        These are product features that are <b>very important</b> for the business, but are backed by services that are in <b>bad health</b>.
      </p>
      <p>
        For example, let's imagine a critical business feature that hasn't changed at all in years. Even if it is in bad shape from a
        technical point of view, why should we worry?
      </p>
      <p>
        The big issue here is that <b>we are in a bad position</b> if we ever see ourselves in the need of applying changes quickly.
      </p>
      <p>
        What happens if there are regulatory changes to be applied in short notice? Or if signing a new life-changing contract depends
        on updating this feature, and we can't commit to it?
      </p>
      <p>
        The key takeaway here is, invest enough resources in keeping your key services in good condition, to avoid a meltdown when it is too late.
      </p>
      <p>
        In the last group, we have those that should open a conversation around: <b>is keeping support for these features even worth it?</b>
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-reconsider-value.png" />
        <span>Why are we wasting resources on this?</span>
      </div>
      <p>
        These are features that are seldom used, not really relevant for the business, and are backed by services that are in bad shape.
      </p>
      <p>
        Seeing your team get bogged down by taking care of large numbers of bugs or incidents related to this kind of features might sound familiar.
      </p>
      <p>
        In these cases, it might be worth to raise a discussion around the feasibility of directly stop supporting them.
      </p>
      <p>
        Depending on the feature, there could be workarounds to them and users can be redirected to do the same job in more steps or in a different way.
        Other features might be impossible to replace by workarounds.
      </p>
      <p>
        Sometimes, even if seldom used, these are mandatory in order to fulfil important contracts.
        Every case is different, but this signals a discussion is worth raising.
      </p>
      <p>
        The key takeway to raise is: what is costing the business more money? Keeping these features alive, or the value generated by them?
      </p>
    </article>

    <article>
      <h2>Session #2 - Example case study</h2>
      <p>
        After reviewing how to fill the technical / business health map, and how to interpret its results, let's review
        the example case study and see what we find.
      </p>
      <p>
        Let's assume there are four features backed by the services we enumerated in the first session:
        <ul>
          <li><b>Purchase</b> &rarr; Web-app + Billing</li>
          <li><b>Search</b> &rarr; Web-app + Search</li>
          <li><b>Rate</b> &rarr; Web-app + Ratings</li>
          <li><b>Recommend to a Friend</b> &rarr; Web-app + Ratings</li>
        </ul>
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-case-study.png" />
        <span>Completed service health map (numerical score added for clarity).</span>
      </div>
      <p>
        The <b>purchase products</b> and <b>search products</b> are the core features of our e-commerce, and the business can't operate without
        them.
      </p>
      <p>
        <b>Rating products</b> is a nice to have feature, many users make use of it and it helps customers navigate the available listings in the platform.
      </p>
      <p>
        Last, <b>recommending products to a friend</b> was an experimental feature and isn't almost at all, besides for a vocal minority of users.
      </p>
      <p>
        What results can we extract from the map?
        <ul>
          <li><b>Purchase Products</b>
            <ul>
              <li>We have a <b>ticking time bomb</b> in our hands.</li>
              <li>This is the <b>core</b> feature of our business.</li>
              <li>However, it's backed by the billing service which has <b>severe issues</b>.</li>
            </ul>
          </li>
          <li><b>Search Products</b>
            <ul>
              <li>Nothing to worry about!</li>
              <li>Backed by services in good health.</li>
              <li>Core business feature with high value.</li>
            </ul>
          </li>
          <li><b>Rate Products</b>
            <ul>
              <li>It's a nice to have feature, but not a business core feature.</li>
              <li>At the same time, it's backed by a service in terrible state.</li>
            </ul>
          </li>
          <li><b>Recommend Product to a Friend</b>
            <ul>
              <li>Has very little business value.</li>
              <li>It's a major source of technical headaches for the team.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        Now we know where we stand, the problems we have in our hands and which should be attended more urgently.
      </p>
    </article>

    <article>
      <h2>Tracing an action plan</h2>
      <p>
        With these results, we have pinpoint which are the critical points across our services that should be fixed
        to ensure business continuity without ugly surprises.
      </p>
      <p>
        The goal with these workshops is being able to create the plan to get out of this situation. The data we've
        collected tells us <b>where</b> should we start, but not <b>what</b> to do.
      </p>
      <p>
        The steps you will have to take as a team to fix the problems in your services and product features will be vary greatly
        depending on the kind of problems you have at hand.
      </p>
      <p>
        To illustrate through an example, we can take the conclusions from the example case study. Let's begin
        with the <b>purchasing products</b> feature.
      </p>
      <p>
        Going back to the results of the first workshop (service health map), we can trace a plan to ensure the service backing
        this feature, the <b>billing service</b>, has the expected quality.
      </p>
      <p>
        The service has severe extendability and test coverage issues, plus security problems. If we want to ensure business continuity,
        a good starting point could be:
        <ul>
          <li>Improving its test coverage to avoid regressions when adding new features or fixing bugs.</li>
          <li>Using the newly improved test coverage, refactor the codebase to facilitate its extendability.</li>
          <li>Resolve the ongoing security issues, before we get a severe incident in our hands.</li>
        </ul>
      </p>
      <p>
        We can plan these improvements as part of our iterations, leaving some headroom so we can advance steadily. Doing work that doesn't
        bring nothing new to the table might raise some eyebrows, but that's why we have the data to back these decisions.
      </p>
      <p>
        The message is that we don't want to improve our software for the sake of doing so: we want to ensure <b>business continuity</b>.
      </p>
      <p>
        Regarding the other two pain-points, the <b>rate products</b> and <b>recommend product to a friend</b> features, we can apply the
        same kind of principles as explained in the previous section.
      </p>
      <p>
        The <b>ratings service</b> has abysmal quality and a very high failure rate. Either we improve it, or we discard the features that are not
        that relevant to the business. The key resides in evaluating the tradeoffs from both options.
      </p>
    </article>

    <article>
      <h2>Reflecting on the results</h2>
      <p>
        In our particular case, this workshop really helped us understand in what situation we were at and put us in the right mindset to
        start working on our ongoing issues.
      </p>
      <p>
        At times where there are too many problems on the table, having data neatly laid out helps a lot with creating a clean mental
        model and making better decisions.
      </p>
      <p>
        Since this was the first run of the workshop, there were many points that were refined as we progressed. What I describe here is the
        refined result of what we went through.
      </p>
      <p>
        After sharing the workshop with other teams within our organization, they tried it out and had positive results. Adapting it to their
        needs was key.
      </p>
      <p>
        If you are going through a similar situation in your organization, I hope this serves as inspiration to attempt this workshop with your team
        and get the ball rolling.
      </p>
      <p>
        And of course, if you try it out and do anything differently, or have any ideas on how to improve it, I would love to hear them.
      </p>
    </article>
    <hr />
    <article>
      <p>
        If you've made it this far, thank you <b>so much</b> for reading!
      </p>
      <p>
        If you have any comments, suggestions or want to discuss any of the topics in this article,
        feel free to reach out through any of the channels in the top of the page.
      </p>
    </article>

    <footer><a href="./index.html">Home</a></footer>

    <script>
    hljs.highlightAll();

    const resizeIframes = () => {
      const isPortrait = window.matchMedia("(orientation: portrait)").matches;
      document.querySelectorAll("iframe").forEach((node) => {
        const contentWidth = document.querySelector("header").scrollWidth;
        const targetWidth = isPortrait ?
          contentWidth :
          contentWidth * 0.5;
        const targetHeight = targetWidth * 0.7;

        node.width = targetWidth;
        node.height = targetHeight;
      });
    };

    const makeResourcesClickable = () => {
      document.querySelectorAll("img").forEach((node) => {
        node.addEventListener("click", () => {
          window.open(node.src);
        });
      });
    };

    window.addEventListener("resize", resizeIframes);
    resizeIframes();
    makeResourcesClickable();
    </script>
  </body>
</html>
