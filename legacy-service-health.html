<html lang="en">
  <head>
    <meta property="og:title" content="César González Segura - Article title">
    <meta property="og:description" content="Article description">
    <meta property="og:image" content="https://cesargonzalez.dev/photo.webp">
    <meta property="og:url" content="https://cesargonzalez.dev">
    <meta name="description" content="César González Segura Personal - Website">
    <meta charset="utf-8"/>
    <title>César González</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="./article.css">
  </head>
  <body>
    <header>
      <img src="images/photo.webp" alt="My Picture" />
      <h1>César González Segura</h1>
    </header>

    <div class="contact">
      <a aria-label="Telegram Contact" href="https://t.me/cegonse"><img src="images/telegram.webp" alt="Telegram Logo"></a>
      <a aria-label="LinkedIn Profile" href="https://www.linkedin.com/in/cesar-gonzalez-segura/"><img src="images/linkedin.webp" alt="LinkedIn Logo"></a>
      <a aria-label="Email Contact" href="mailto:contact@cesargonzalez.dev"><img src="images/email.webp" alt="Email Symbol"></a>
      <a aria-label="GitHub Profile" href="https://github.com/cegonse"><img src="images/github.webp" alt="GitHub Logo"></a>
    </div>

    <h2>A workshop to help your team navigate legacy software</h2>
    <p class="italic">Posted February 17th 2026</p>
    <p class="italic">Did you enjoy this article? Follow the conversation on <a>LinkedIn</a></p>

    <article>
      <h2>The importance of properly maintaining legacy software</h2>
      <p>
        If you have been around software engineering communities for a while, you will be aware of the reputation of legacy software.
        Many engineers associate it with fragile codebases, built over years of decisions nobody understands anymore.
      </p>
      <p>
        The reality is that in many cases, legacy software is what keeps businesses afloat. Silently processing
        millions of transactions on a daily basis, generating revenue and providing value to users.
      </p>
      <p>
        As time passes, software starts decaying. Team members come and go, institutional knowledge gets lost and its supporting
        architecture dillutes.
      <p>
        At the same time, there's little incentive from the business side to modernize it. If it keeps working, why spend precious
        resources on modernizing it? Well, because at some point, it will stop working.
      </p>
      <p>
        In one of my previous teams, we were facing way too many issues at the same time. We were in charge of many legacy products and services, and
        we had to modernize them fast.
      </p>
      <p>
        With our limited time and resources, we wouldn't be able to do it all on time. So the question was: what's prioritary?
      </p>
      <p>
        As a way to help us get out of the deadlock we were in, we built this workshop that helped us define
        what had to be fixed, and in which order, to ensure we would be able to adapt on time to the business needs.
      </p>
    </article>

    <article>
      <h2>Introduction to the workshop</h2>
      <p>
        The objective of this workshop is to help your team:
        <ul>
          <li>Understand the most pressing issues with your legacy services are.</li>
          <li>Plan what to fix first to ensure business continuity.</li>
          <li>Share knowledge about legacy systems with the whole team.</li>
        </ul>
      </p>
      <p>
        The workshop consists of <b>two sessions</b>, taking around <b>two hours</b> each. It's important
        that most of your team members are able to attend the workshop.
      </p>
      <p>
        Preparation work for both sessions can take around another <b>2-3 hours</b> before each session. You can do
        the prep work by yourself with help from other team members.
      </p>
      <p>
        To get the most out of the workshop, it's best to limit the number of attendees to under <b>10 people</b>.
        You want to ensure everybody has enough time and space to contribute.
      </p>
      <p>
        The purpose of the two sessions is:
        <ul>
          <li><b>Mapping service health</b>
            <ul>
              <li>Understanding the technical health of the services maintained by your team.</li>
              <li>Outcome &rarr; Know the health of your services.</li>
            </ul>
          </li>
          <li><b>Mapping business / service relevance</b>
            <ul>
              <li>Understand the business relevance of your team's services, and detect problems.</li>
              <li>Outcome &rarr; Know what services are worth fixing from a business point of view.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        By the end of the workshop, you should have a picture of what product features could be at risk due to problems
        in the legacy services backing them, and which should be solved first.
      </p>
    </article>

    <article>
      <h2>Setting up an example case study</h2>
      <p>
        To illustrate how the workshop could be run with a concrete example, let's introduce an imaginary product and its stack.
      </p>
      <p>
        Let's assume we have an e-commerce platform, backed by several in-house services and other off-the-shelf or external SaaS
        services.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/case-study.png" />
        <span>Service stack in our platform.</span>
      </div>
      <p>
        In this workshop, we will focus on <b>in-house services</b>, ignoring off-the-shelf and external components of the
        system.
      </p>
    </article>

    <article>
      <h2>Session #1 - Mapping service health</h2>
      <p>
        This session helps the team reflect about the services they take care of, and evaluate their health.
        A 2~3 hour slot should suffice for this session, but you will have to prepare it first.
      </p>
      <p>
        The team will have to describe their services through <b>metrics</b>. These metrics can be either <b>quantitative</b>
        or <b>qualitative</b>.
      </p>
      <p>
        Preparing the workshop materials with a team member who has deep knowledge about your services helps a lot.
        They can provide insights to define meaningful metrics to describe them.
      </p>
      <p>
        Metrics have a score, mapping their health from best case to worst case. We used a simple scoring system ranging 1~4,
        together with a color code to help us map metrics in a digital whiteboard:
        <ul>
          <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>) - Ideal</b> &rarr; As good as it can be.</li>
          <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>) - Acceptable</b> &rarr; Rough around its edges, but perfectly manageable.</li>
          <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) - Worrying</b> &rarr; Problems are becoming apparent, and could get worse.</li>
          <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>) - Danger</b> &rarr; We are not ready to react if something unexpected happens.</li>
        </ul>
      </p>
      <p>
        Once these metrics are ready, we'll set-up a service / health matrix in our digital whiteboard to let the team discuss and
        assign a score to each service.
      </p>
      <p>
        Start by creating a matrix with all the services, and one by one, let the team
        decide which score makes more sense for each:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/empty-matrix.png" />
        <span>Start with all your services, and the metrics you have chosen.</span>
      </div>
      <p>
        Then, start a discussion for each service and start assigning a score to each metric. As we'll see in the next section,
        some metrics will be objectively measurable, while others will raise a more heated debate.
      </p>
      <p>
        Assign a score from 1~4 (green to black, or the color palette of your liking), and finally assign the aggregated score for each of
        them. A simple scoring formula that worked well is <span class="inline-code">ceil(avg([scores]))</span>.
      </p>
      <p>
        This formula highlights bad scores, ensuring that scores of services with poorly scoring metrics get in the spotlight. This helps to bring attention
        to services that need it.
      </p>
      <p>
        Let's add the scores for the metrics and calculate the score for each metric:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/filled-matrix.png" />
        <span>All services have been evaluated and assigned a score.</span>
      </div>
      <p>
        With these results, we have a bird's eye view of the technical health of our services, split by metric. To make it work,
        we will have to define which metrics can be used to measure its health.
      </p>
      <p>
        In the next section, you'll find some example metrics that you can use in your own workshop. And later,
        we'll review the example case study with a set of metrics.
      </p>
    </article>
    <article>
      <h2>Quantitative metrics</h2>
      <p>
        Quantitative metrics are those that can be measured from objective data. These can be
        useful to determine how far your team's services are from the goals set by your organization,
        upcoming regulatory requirements, industry standards, etc.
      </p>
      <p>
        For each metric, you should define the measurable points and/or thresholds to set
        the metric's score. For example, consider a metric with four yes/no questions, where "yes" is
        positive (you want to fulfill all items).
      </p>
      <p>
        In this case, having a "yes" answer to all points would score a 1 (<span style="color: rgb(196, 246, 123)">▣</span>),
        having a "yes" to three out of four would score a 2 (<span style="color: rgb(252, 164, 0)">▣</span>), and so on.
      </p>
      <p>
        Here are some examples you can use as a starting point. Some of these metrics can be answered on a
        yes/no basis, others require gathering actual numbers:
      </p>
      <ul>
        <li>
          <b>Obsevability</b>
            <ul>
              <li>Are our systems observable from the outside?</li>
              <li>Are there alarms in place to notify the team in case there is a production failure?</li>
              <li>Is the team able to understand the current state of the system by checking logs and traces?</li>
              <li>Are performance measurements available for our systems?</li>
            </ul>
        </li>
        <li>
          <b>Security</b>
          <ul>
            <li>Do our systems use libraries or frameworks with known security vulnerabilities?</li>
            <li>Are we running security scanning processes on our systems?</li>
            <li>If so, is the team actively patching found security vulnerabilities?
            <li>Does the team follow security best practices? (masking PII data in logs, properly storing sensitive data and secrets, etc)</li>
          </ul>
        </li>
        <li>
          <b>Up-to-dateness</b>:
          <ul>
            <li>Are any of our systems using frameworks or libraries out of their maintenance window?</li>
            <li>Are any systems using frameworks that could complicate updating to modern versions if needed? (ex. NET Framework, PHP 5...)</li>
            <li>Do our systems comply with all applicable regulations?</li>
          </ul>
        </li>
        <li>
          <b>Scalability</b>
            <ul>
              <li>Is the team aware of how our services perform under load?</li>
              <li>Does the architecture of the systems allow scaling them up when required?</li>
              <li>Can the team scale them autonomously, or do we need support or permission from other teams?</li>
              <li>Are there auto-scaling measures in place?</li>
              <li>Do we perform regular load testing on our services?</li>
          </ul>
        </li>
        <li>
          <b>Failure Rate</b>
          <ul>
            <li>How often does the service suffer from critical bugs?</li>
            <li>Does the service go down? If so, how long does its recovery take?</li>
            <li>Is the service part of the critical path? Can the business continue operating during downtime?</li>
          </ul>
        </li>
      </ul>
    </article>
    <article>
      <h2>Qualitative metrics</h2>
      <p>
        Qualitative metrics work great to help your team assess how they "feel" when working with
        their systems. Discussing metrics without factual data to back them might seem counterproductive,
        but in some cases trying to express all problems with hard numbers complicates debate.
      </p>
      <p>
        Some qualitative metrics you can use are:
      </p>
      <ul>
        <li>
          <b>Extendability</b>
          <ul>
            <li>How easy is it to add new features to our service?</li>
            <li>Are we held back by its architecture and/or design?</li>
            <li>Do we avoid extending the service beyond surgical changes due to accumulated tech debt?</li>
          </ul>
        </li>
        <li>
          <b>Testing quality</b>
          <ul>
            <li>Are the tests backing our service easy to read and understand?</li>
            <li>Do we trust the results of our tests?</li>
            <li>Is it easy to create new tests?</li>
            <li>Does our test codebase break easily when adding new features?</li>
          </ul>
        </li>
      </ul>
      <p>
        There are always objective ways of measuring these metrics with hard data. But in many
        cases the debate around these topics is much more valuable to understand the general
        "vibe" of working with these services than hard numbers by themselves.
      </p>
      <p>
        For example, you could assess testing quality quantitatively by measuring the code base's coverage,
        the presence of flaky tests or the time required to execute a full test suite.
      </p>
      <p>
        But if the architecture supporting these tests makes creating new tests or extending existing
        tests a chore, the team will have an easier time expressing this pain point from a feelings
        point of view. Even if from a numbers perspective, everything seems to be okay.
      </p>
    </article>

    <article>
      <h2>Session #1 - example case study</h2>
      <p>
        Let's go back to our example case study, and define which metrics are most relevant for our services, and how
        to measure them.
      </p>
      <p>
        Our service matrix will consist of the four key services our team manages (excluding off-the-shelf
        components). To measure their health, we will use four metrics, two qualitative and two quantitative:
        <ul>
          <li>
            <b>Extendability</b>: How easy is it to extend the service with new features?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Extending the service with new functionality is seamless. Adding new features and improvements takes minimal time.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; Adding new functionalities can get cumbersome without refactoring first.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; The codebase has grown without regard to its architecture, and adding new features is time consuming and a potential risk.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; Adding new features is very time consuming, and requires extensive prior clean up through refactoring.</li>
            </ul>
          </li>
          <li>
            <b>Testing Quality</b>: How good are the tests supporting development of the service?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Tests are well maintained, comprehensive and robust. Includes functional and non-functional testing, stress tests, etc...</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; There is decent test coverage, but their design makes them prone to breaking.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; There are some tests, but not coverage isn't good enough to allow changes without risk.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; There are no tests, and adding new tests would take a lot of effort at this point.</li>
            </ul>
          </li>
          <li>
            <b>Failure Rate</b>: How often does the system fail? How long does it take to recover?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Less than 5 critical incidents YoY. MTTR under 30 minutes.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; ~10 critical incidents YoY. MTTR under 30 minutes.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; ~10 critical incidents YoY. MTTR over 30 minutes.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; ~4 critical incidents / week. MTTR over 30 minutes.</li>
            </ul>
          </li>
          <li>
            <b>Security & Up-to-Date</b>: Are its dependencies (including frameworks) up to date? Have security vulnerabilities been detected?
            <ul>
              <li><b>1 (<span style="color: rgb(196, 246, 123)">▣</span>)</b> &rarr; Zero critical security vulnerabilities. Dependencies use latest stable versions.</li>
              <li><b>2 (<span style="color: rgb(252, 164, 0)">▣</span>)</b> &rarr; Zero critical security vulnerabilities. Dependencies use older but still supported stable versions.</li>
              <li><b>3 (<span style="color: rgb(255, 88, 88)">▣</span>) </b> &rarr; Active critical security vulnerabilities present. Dependencies use older but still supported stable versions.</li>
              <li><b>4 (<span style="color: rgb(0, 0, 0); -webkit-text-stroke-color: rgb(236, 233, 230); -webkit-text-stroke-width: 0.5px;">▣</span>)</b> &rarr; Active critical security vulnerabilities present. Dependencies use deprecated versions.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        Now that we have a set of metrics to measure the health of our team's services, we can build the service / metric health matrix
        and start filling in the score for each metric.
      </p>
      <p>
        We can leverage logs, performance metrics, reports from the package manager, security vulnerability analysis tools and post-mortem documentation
        from previous failures and incidents, etc...
      </p>
      <p>
        On the other hand, extendability and testing quality metrics will involve a deeper discussion between the developers in the team. How was the
        experience of building the latest features in each service? Was it easy, or did we try to avoid it at all costs?
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/case-study-filled-matrix.png" />
        <span>Completed service health map (numerical score added for clarity).</span>
      </div>
      <p>
        From what we've gathered, our <b>web-app</b> and <b>search service</b> are in a good
        enough shape, at an acceptable point for a production app.
      </p>
      <p>
        However, the <b>billing</b> and <b>ratings</b> services are in trouble. The <b>ratings service</b> has strong ongoing issues, with a very high failure rate, lurking
        security issues and no useful test base to help us overcome these problems.
      </p>
      <p>
        Should we jump in and plan the work to get these services into shape during next iterations? First, we should ask ourselves: <b>is it worth it?</b>
      </p>
    </article>

    <article>
      <h2>Session #2 prep - Considering bussiness importance</h2>
      <p>
        If we only cared about perfection, it would be great to fix all the problems we've found. But with limited time, capacity and rapidly
        switching objectives, we have to compromise.
      </p>
      <p>
        Before starting to plan how to fix the issues we've found, first we had to understand if doing it makes sense from a business perspective.
      </p>
      <p>
        To do so, we scheduled a short follow-up session with product and business specialists from our team.
      </p>
      <p>
        We listed the product features that are backed by our team's services, and sorted all features by <b>business importance</b>,
        using a numerical scale as we did with the technical services.
      </p>
      <p>
        Ranging from 1 (<span style="color: rgb(196, 246, 123)">▣</span>), meaning a feature that's very important for the business, to
        3 (<span style="color: rgb(255, 88, 88)">▣</span>), meaning it's just a nice-to-have.
      </p>
      <p>
        To support this classification, we used a variety of metrics our product experts had in hand. Some of them were:
        <ul>
          <li><b>Usage numbers</b>
            <ul>
              <li>How many users do actively use the feature? (DAU / MAU)</li>
              <li>Are the users making use of the feature important for the business? (paying customers, important contracts...)</li>
            </ul>
          </li>
          <li><b>Revenue Generation</b>
            <ul>
              <li>What is the impact of the feature in terms of revenue, regardless of usage?</li>
              <li>What would happen to revenue if we stopped supporting it?</li>
            </ul>
          </li>
          <li><b>Compliance & Strategical Opportunity</b>
            <ul>
              <li>Is this feature a regulatory requirement in key markets?</li>
              <li>Does supporting it keep users from leaving the platform?</li>
              <li>Is it a requirement for an important contract with a major customer?</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        With the list of features in hand, sorted by their business relevance, we can prepare the next workshop session: matching each
        product feature with its backing service.
      </p>
    </article>

    <article>
      <h2>Session #2 - Evaluating technical / business health</h2>
      <p>
        In this session, we will complete linking the business relevance of each product feature to the services supporting them.
      </p>
      <p>
        To build this mapping, the steps we follow are:
        <ol>
          <li>Build a matrix with all services as rows and product features as columns.</li>
          <li>
            Sort both services and features with <b>descending health and relevance</b> (top left corner for best, bottom right corner for worst).
            <ul><li>Use the color key to help visualize health and relevance at a glance.</li></ul>
          </li>
          <li>
            Place a <b>pin</b> in each cell where a product feature is backed by a service.
            <ul><li>In the following example, Feature A is backed by Services 1 and 2.</li></ul>
          </li>
        </ol>
      </p>
      <p>
        The resulting matrix will look like this:
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-matrix.png" />
        <span>Mapping product features against services.</span>
      </div>
      <p>
        With this information at our disposal, it's time to start making decisions on what's worth fixing and what's not. We'll focus
        on looking for features that have:
        <ul>
          <li>Good health + business relevance.</li>
          <li>Bad health + business relevance.</li>
          <li>Bad health + questionable business relevance.</li>
        </ul>
      </p>
      <p>
        First, we have the best-case scenario: features that are relevant for the business, which are supported by
        services that have good health.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-all-good.png" />
        <span>No need to worry about these.</span>
      </div>
      <p>
        We can safely ignore these and just keep up the good work. If there are any new features, unexpected regulatory changes
        or required updates, we will be able to handle them without problems.
      </p>
      <p>
        Next, let's focus on the <b>danger zone</b>.
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-danger-zone.png" />
        <span>We could be in trouble if something goes wrong with these.</span>
      </div>
      <p>
        These are product features that are relevant for the business, but are backed by services that are in <b>poor health</b>.
        We should be looking forward to fixing the issues we've detected as soon as possible.
      </p>
      <p>
        In the last group, we have those that should open a conversation around: <b>is supporting these features even worth it?</b>
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-reconsider-value.png" />
        <span>Why are we investing resources on this?</span>
      </div>
      <p>
        These are features that are not really relevant for the business, and are backed by services that are in bad shape.
      </p>
      <p>
        Seeing your team get bogged down with bugs or incidents related to this kind of features might sound familiar.
        In these cases, raising a discussion around the feasibility of directly stopping support for them might be worth it.
      </p>
      <p>
        The key takeway to raise is: what is costing the business more money? Keeping these features alive, or the value generated by them?
      </p>
    </article>

    <article>
      <h2>Session #2 - Example case study</h2>
      <p>
        Let's assume there are four features backed by the services we enumerated in the first session:
        <ul>
          <li><b>Purchase</b> &rarr; Web-app + Billing</li>
          <li><b>Search</b> &rarr; Web-app + Search</li>
          <li><b>Rate</b> &rarr; Web-app + Ratings</li>
          <li><b>Recommend to a Friend</b> &rarr; Web-app + Ratings</li>
        </ul>
      </p>
      <div class="image fit-20">
        <img src="./images/legacy-article/business-service-case-study.png" />
        <span>Completed service health map (numerical score added for clarity).</span>
      </div>
      <p>
        The <b>purchase products</b> and <b>search products</b> are the core features of our e-commerce, and the business can't operate without
        them.
      </p>
      <p>
        <b>Rating products</b> is a nice-to-have feature, many users make use of it and it helps customers navigate the listings on the platform.
      </p>
      <p>
        Last, <b>recommending products to a friend</b> was an experimental feature and isn't used almost at all, besides for a minority of users.
      </p>
      <p>
        What results can we extract from the map?
        <ul>
          <li><b>Purchase Products</b>
            <ul>
              <li>We have a problem in our hands.</li>
              <li>This is the <b>core</b> feature of our business.</li>
              <li>However, it's backed by the billing service which has <b>severe issues</b>.</li>
            </ul>
          </li>
          <li><b>Search Products</b>
            <ul>
              <li>Nothing to worry about!</li>
              <li>Backed by services in good health.</li>
              <li>Core business feature with high value.</li>
            </ul>
          </li>
          <li><b>Rate Products</b>
            <ul>
              <li>It's a nice-to-have feature, but not a business core feature.</li>
              <li>At the same time, it's backed by a service in a terrible shape.</li>
            </ul>
          </li>
          <li><b>Recommend Product to a Friend</b>
            <ul>
              <li>Has very little business value.</li>
              <li>It's a major source of technical headaches for the team.</li>
            </ul>
          </li>
        </ul>
      </p>
      <p>
        Now we know where we stand, the problems we have in our hands and which should be addressed urgently.
      </p>
    </article>

    <article>
      <h2>Tracing an action plan</h2>
      <p>
        The goal of this workshop is creating a plan to get out of this situation. The data we've
        collected tells us <b>where</b> we should start, but not <b>what</b> to do.
      </p>
      <p>
        The steps you will have to take as a team to fix the problems in your services and product features will vary greatly
        depending on the kind of problems you've found.
      </p>
      <p>
        To illustrate through an example, take the conclusions from the example case study. Let's begin
        with the <b>purchasing products</b> feature.
      </p>
      <p>
        Going back to the results of the first workshop (service health map), we can trace a plan to ensure the service backing
        this feature, the <b>billing service</b>, has the expected quality.
      </p>
      <p>
        The service has severe extendability and test coverage issues, plus security problems. If we want to avoid future problems,
        a good starting point could be:
        <ul>
          <li>Improving its test coverage to avoid regressions when adding new features or fixing bugs.</li>
          <li>Using the newly improved test coverage, refactor the codebase to facilitate its extendability.</li>
          <li>Resolve the ongoing security issues, before we get a severe incident on our hands.</li>
        </ul>
      </p>
      <p>
        We can plan these improvements as part of our iterations, leaving some headroom so we can advance steadily. We can use the data discovered
        during this workshop to negotiate the budget.
      </p>
      <p>
        Regarding the other two pain-points, the <b>rate products</b> and <b>recommend product to a friend</b> features, we can apply the
        same steps.
      </p>
      <p>
        The <b>ratings service</b> has abysmal quality and a very high failure rate. Either we improve it, or we discard the features that are not
        relevant to the business. The key resides in evaluating the tradeoffs from both options.
      </p>
    </article>

    <article>
      <h2>Reflecting on the results</h2>
      <p>
        In our case, this workshop helped us understand where we standed and put us in the right mindset to
        start working on our ongoing issues.
      </p>
      <p>
        At times when there are too many problems on the table, having data neatly laid out helps a lot with creating a clean mental
        model and making better decisions.
      </p>
      <p>
        After sharing the workshop with other teams within our organization, they tried it out and had positive results. Adapting it to their
        needs was key.
      </p>
      <p>
        If you are going through a similar situation in your organization, I hope this serves as inspiration to attempt this workshop with your team
        and get the ball rolling.
      </p>
      <p>
        And of course, if you try it out and do anything differently, or have any ideas on how to improve it, I would love to hear them.
      </p>
    </article>
    <hr />
    <article>
      <p>
        If you've made it this far, thank you <b>so much</b> for reading!
      </p>
      <p>
        If you have any comments, suggestions or want to discuss any of the topics in this article,
        feel free to reach out through any of the channels in the top of the page.
      </p>
    </article>

    <footer><a href="./index.html">Home</a></footer>

    <script>
    hljs.highlightAll();

    const resizeIframes = () => {
      const isPortrait = window.matchMedia("(orientation: portrait)").matches;
      document.querySelectorAll("iframe").forEach((node) => {
        const contentWidth = document.querySelector("header").scrollWidth;
        const targetWidth = isPortrait ?
          contentWidth :
          contentWidth * 0.5;
        const targetHeight = targetWidth * 0.7;

        node.width = targetWidth;
        node.height = targetHeight;
      });
    };

    const makeResourcesClickable = () => {
      document.querySelectorAll("img").forEach((node) => {
        node.addEventListener("click", () => {
          window.open(node.src);
        });
      });
    };

    window.addEventListener("resize", resizeIframes);
    resizeIframes();
    makeResourcesClickable();
    </script>
  </body>
</html>
