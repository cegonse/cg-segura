<html lang="en">
  <head>
    <meta property="og:title" content="César González Segura - Applying snapshot testing to game development and 3D graphics">
    <meta property="og:description" content="Explore how to combine off-screen rendering and software rendering to build snapshot tests for complex 3D visualizations">
    <meta property="og:image" content="https://cesargonzalez.dev/photo.webp">
    <meta property="og:url" content="https://cesargonzalez.dev">
    <meta name="description" content="César González Segura - Applying snapshot testing to game development and 3D graphics">
    <meta charset="utf-8"/>
    <title>César González</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
      html {
        color: rgb(236, 233, 230);
        background-color: #1b2733;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        width: 100%;
      }

      @media (orientation: landscape) {
        body {
          margin-left: 20%;
          margin-right: 20%;
        }
      }

      @media (orientation: portrait) {
        body {
          margin-left: 5%;
          margin-right: 5%;
        }
      }

      header {
        display: flex;
        flex-direction: row;
        align-items: center;
      }

      @media (orientation: landscape) {
        header img {
          height: 2em;
          width: 2em;
          border-radius: 50%;
          margin-right: 1em;
        }
      }

      @media (orientation: portrait) {
        header {
          display: flex;
          flex-direction: column;
          align-items: center;
        }

        header img {
          height: 5em;
          width: 5em;
          border-radius: 50%;
        }
      }

      h2 {
        font-size: 2.5em;
        margin-bottom: 0.1em;
      }

      h3 {
        font-size: 1.5em;
        margin-bottom: 0;
      }

      p {
        text-align: justify;
        font-size: 1.2em;
      }

      li {
        font-size: 1.2rem;
        margin-top: 0.5rem;
        margin-bottom: 0.5rem;
      }

      a {
        color: #758CFF;
        text-decoration: underline;
        padding-bottom: 0;
      }

      a:hover {
        color: #4D6AFF;
        text-decoration: underline;
      }

      .image {
        display: flex;
        flex-direction: column;
        align-items: center;

        img {
          border-radius: 8px;
          object-fit: contain;
          width: 100%;
          height: auto;
          margin-top: 1em;
        }

        video {
          border-radius: 8px;
          object-fit: contain;
          width: 100%;
          height: auto;
          margin-top: 1em;
        }

        span {
          margin-top: 1em;
          margin-bottom: 0.5em;
          font-style: italic;
        }
      }

      @media (orientation: landscape) {
        .fit-20 {
          margin-left: 20%;
          margin-right: 20%;
        }
      }

      article {
        margin-top: 4rem;
        margin-bottom: 4rem;
      }

      article > p:first-child {
        margin-top: 2em;
      }

      article > a {
        display: inline-block;
      }

      article > h2 {
        font-size: 2em;
      }

      pre > code {
        border-radius: 8px;
      }

      pre {
        width: 100%;
      }

      iframe + span {
        margin-top: 0.5em;
        margin-bottom: 0.5em;
        font-style: italic;
        text-align: center;
      }

      video {
        border-radius: 8px;
      }

      video + span {
        margin-top: 0.5em;
        margin-bottom: 0.5em;
        font-style: italic;
        text-align: center;
      }

      pre + span {
        margin-bottom: 0.5em;
        font-style: italic;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        text-align: center;
      }

      @media (orientation: landscape) {
        article > pre {
          margin-left: 10%;
          margin-right: 10%;
          margin-top: 2em;
          margin-bottom: 2em;
        }
      }
      @media (orientation: portrait) {
        article > pre {
          margin-top: 2em;
          margin-bottom: 2em;
        }
      }

      article > div {
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      article > div > table {
        text-align: center;
        border-collapse: separate;
        border-spacing: 2em 0;
        font-size: 1.2em;
      }

      table + span {
        font-style: italic;
        margin-top: 1em;
        margin-bottom: 1em;
        text-align: center;
      }

      .italic {
        font-style: italic;
      }

      .contact {
        display: flex;
        flex-direction: row;
      }

      .contact img {
        height: 2em;
        margin-right: 1em;
      }

      .contact a:hover {
        transform: scale(1.05);
        filter: brightness(0.8)
      }

      @media (orientation: portrait) {
        .contact {
          justify-content: space-around;
        }

        .contact img {
          height: 48px;
        }
      }

      footer {
        width: 100%;
        height: 3em;
        display: flex;
        flex-direction: row;
        justify-content: center;
        align-items: center;
        background-color: #263748;
        border-radius: 8px;
        margin-top: 4em;
        margin-bottom: 1em;
      }

      @media (orientation: portrait) {
        footer {
          font-size: 1.5em;
          border-radius: 32px;
        }
      }

      .info {
        background-color: #42586e;
        border-radius: 8px;
        padding: 1em;
        display: block !important;
        margin-top: 1rem;
        margin-bottom: 1rem;
      }

      .inline-code {
        font-family: 'Courier New', Courier, monospace;
        font-size: 0.8em;
        background-color: #27292b;
        color: #ffa826;
        border-radius: 4px;
        padding: 2px;
      }

      @media print {
        p {
          color: black;
        }

        h1 {
          color: black;
        }

        h2 {
          color: black;
        }

        li {
          color: black;
        }

        a {
          color: black;
          text-decoration: none;
        }

        footer {
          display: none;
        }

        .info {
          color: black;
        }

        iframe {
          display: none;
        }

        pre + span {
          color: black;
        }

        iframe + span {
          display: none;
        }

        video + span {
          display: none;
        }

        video {
          display: none;
        }

        .contact {
          display: none;
        }

        .image {
          span {
            color: black;
          }
        }
      }
    </style>
  </head>
  <body>
    <header>
      <img src="images/photo.webp" alt="My Picture" />
      <h1>César González Segura</h1>
    </header>

    <div class="contact">
      <a aria-label="Telegram Contact" href="https://t.me/cegonse"><img src="images/telegram.webp" alt="Telegram Logo"></a>
      <a aria-label="LinkedIn Profile" href="https://www.linkedin.com/in/cesar-gonzalez-segura/"><img src="images/linkedin.webp" alt="LinkedIn Logo"></a>
      <a aria-label="Email Contact" href="mailto:contact@cesargonzalez.dev"><img src="images/email.webp" alt="Email Symbol"></a>
      <a aria-label="GitHub Profile" href="https://github.com/cegonse"><img src="images/github.webp" alt="GitHub Logo"></a>
    </div>

    <h2>Applying snapshot testing to game development</h2>
    <p class="italic">Posted July 30th 2025</p>
    <p class="italic">Did you enjoy this article? Follow the conversation on <a>LinkedIn</a></p>
    <article>
      <div class="info">ℹ️ All code used to prepare this article is available on GitHub at <a href="https://github.com/cegonse/offscreen-rendering-tests">cegonse/offscreen-rendering-tests</a></div>
      <p>
        Getting a game to feel, look and play as intended is hard. There's a delicate balance that must be taken care of at all times.
        Any seemingly tiny change could have enormous consequences on how a game looks and feels.
      </p>
      <p>
        Applying testing techniques to games as if they were any software product does not always cut it. There are nuances in how a game
        <i>feels</i> that are hard to capture with unit tests.
      </p>
      <p>
        Systems are so complex and interconnected that it gets tough to keep track of how changes to shared modules might
        impact the entire game.
      </p>
      <p>
        Manual testing the whole game every time a change is made to the core mechanics or graphics isn't feasible, except for tiny
        games.
      </p>
      <p>
        Those manual tests could be automated too, but as with all heavy-weight full-app tests, building and maintaining them is time-consuming
        and prone to breaking.
      </p>
      <p>
        I wanted to explore a lightweight way to test full subsets of the game, without having to rely on playing the <i>actual</i> game.
      </p>
      <p>
        Can we run small segments of the game and compare the results with stored <i>snapshots</i>, and find out if anything relevant has
        changed?
      </p>
      <p>
        In this article, I've explored how to apply <b>snapshot testing</b> to reduce unintended graphics regressions when building games.
      </p>
      <p>
        There are many scenarios where snapshot testing can be useful, giving us a break from writing heavy acceptance tests. But it also comes
        with several caveats, so it won't be applicable in all cases.
      </p>
    </article>
    <article>
      <h2>Testing and game development</h2>
      <p>
        Since I first started learning about software automated testing, I began thinking about how it could be
        effectively applied to game development.
      </p>
      <p>
        If you have ever developed a videogame, you will have experienced first-hand how complex game development can be.
        Coincidentally, the environment where I've seen less automated testing through my career is game development.
      </p>
      <p>
        Reading from sites like <a href="https://www.gamedeveloper.com/">GameDeveloper</a> and remembering conversations with
        people working on established studios, I learned that many rely on automated QA tests based on macros.
      </p>
      <p>
        These tests auto-play portions of the game from recorded controller inputs and bespoke markers detect whether a goal has
        been reached or not. This requires launching the full game, and as with all QA acceptance tests, they are:
      </p>
      <ul>
        <li>Expensive to implement.</li>
        <li>Prone to breaking upon change.</li>
        <li>Expensive to maintain and to keep them working.</li>
        <li>Require powerful hardware to run (as powerful as the hardware required to run the game itself).</li>
      </ul>
      <p>
        QA acceptance tests are of course necessary and must always be present in any product's testing pyramid. But,
        how could the cost (both building, maintaining and running) of a game's test suite be reduced?
      </p>
      <p>
        A friend of mine introduced me to how <a href="https://store.steampowered.com/app/427520/Factorio">Factorio</a> tests its vast
        number of mechanics, and it rang a bell.
      </p>
      <div>
        <iframe width="auto" height="auto" src="https://www.youtube.com/embed/CgMV2dFFdFE" title="Factorio: Space Age - Graphics mode tests" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <span><a href="https://store.steampowered.com/app/427520/Factorio">Factorio's</a> testing mode resembles what I wanted to achieve</span>
      </div>
      <p>
        What about running smaller integration tests of only a subset of the full game? And all this while avoiding running them on retail hardware?
        For example, directly on GitHub Actions CI runners.
      </p>
      <p>
        The idea I came up with was combining a couple of technical concepts to achieve this, <b>snapshot testing</b> and <b>off-screen rendering</b>.
      </p>
    </article>

    <article>
      <h2>Understanding the concepts</h2>
      <p>
        Before getting started with the details, let's first define what are snapshot testing and off-screen rendering.
      </p>
      <h2>Snapshot Testing</h2>
      <p>
        Instead of attempting to define <b>snapshot testing</b> myself, I'll quote the definition from <a href="https://jestjs.io/docs/snapshot-testing">Jest's documentation</a>:
      </p>
      <div class="info">
        <i>
          <p>
            A typical snapshot test case <b>renders</b> a UI component, takes a <b>snapshot</b>, then <b>compares</b> it to a reference snapshot
            file stored alongside the test.
          </p>
          <p>
            The test will <b>fail if the two snapshots do not match</b>: either the <b>change is unexpected</b>, or the reference snapshot
            <b>needs to be updated</b> to the new version of the UI component.
          </p>
        </i>
      </div>
      <p>
        Even though Jest mentions UI components, this technique can be applied to any resource that should not change unless intended.
      </p>
      <p>
        For example, it works well when comparing CSV or JSON files generated by an integration test. So, why not with images?
      </p>
      <p>
        A game snapshot test could render frames from the game, store the valid result together with the test, and fail the test
        if the rendering results change.
      </p>
      <p>
        To achieve this, the game and its test must be able to:
      </p>
      <ul>
        <li>Render full frames in the testing environment (both local and CI).</li>
        <li>Generate a stable result in every execution, so runs can be compared with the stored snapshot.</li>
      </ul>
      <p>
        The latter point is a challenge in many rendering engines, but depending on the control you have over the rendering
        process it can be tuned to be deterministic between runs.
      </p>
      <p>
        The former can be solved through <b>off-screen rendering</b>, and combining it with <b>software rendering</b> could
        make it work anywhere (like CI runners).
      </p>
      <h2>Off-screen & software rendering</h2>
      <p>
        Since the purpose of graphics rendering in a game is to let players interact with the game, games render their content
        inside a window on the screen.
      </p>
      <p>
        But as you will probably know, that is not the only way to render graphics. The graphics stack doesn't care about
        where the image ends up. It only renders the GPU state into a buffer.
      </p>
      <p>
        One of these buffers (the framebuffer) is used to show the game's rendering on the screen. Additional buffers are used
        to perform multi-pass rendering, full-screen effects, or rendering to textures in the scene itself.
      </p>
      <p>
        If you want a reference of real-world libraries, check Unity's <a href="https://docs.unity3d.com/es/530/Manual/class-RenderTexture.html">RenderTexture</a>
        or OpenGL's <a href="https://www.khronos.org/opengl/wiki/Framebuffer_Object">Frame Buffer Objects</a>.
      </p>
      <p>
        To get the snapshot tests to work, the game's tests will have to be tuned to only draw in <b>off-screen buffers</b>,
        ensuring it works in headless environments (without a windowing system).
      </p>
      <p>
        Getting the game to run in headless mode would be enough to get it to run on our local machine, or on a specialized CI
        runner with a GPU.
      </p>
      <p>
        But in most cases, the CI runners at our disposal do not have GPUs. In fact, most projects run their tests on shared CI runners,
        executing them on a sandboxed, containerized environment.
      </p>
      <p>
        In these cases, a tool we can make use of is <b>software rendering</b>. There are software implementations of graphical
        stacks like <a href="https://mesa3d.org/">Mesa</a>'s implementations of OpenGL and Vulkan.
      </p>
      <p>
        Forcing our game to run with a software rendering backend, instead of the real GPU driver, would let the snapshot tests
        run on any environment.
      </p>
      <p>
        Sadly, software rendering is much, much slower than GPU rendering (we wouldn't need GPUs after all if that wasn't the case!),
        so performance might be a problem down the line.
      </p>
    </article>

    <article>
      <h2>Tooling and frameworks</h2>
      <p>
        I wanted to focus on evaluating snapshot testing as a technique, so avoiding having to fight to get the tooling to work as I wanted was key.
      </p>
      <p>
        In this case, I've selected options that give me enough flexibility to adapt the development environment, rendering backend (to add offscreen rendering
        support) and test code.
      </p>
      <p>
        I finally decided to use C++17 with this stack:
      </p>
      <ul>
        <li>
          <b>Rendering Backend</b>
          <ul>
            <li><a href="https://www.raylib.com/">Raylib</a></li>
            <li><a href="https://www.opengl.org/">OpenGL</a></li>
            <li><a href="https://www.glfw.org/">GLFW</a></li>
            <li><a href="https://gallium.readthedocs.io/en/latest/osmesa.html">libosmesa</a></li>
          </ul>
        </li>
        <li>
          <b>Testing Environment</b>
          <ul>
            <li><a href="https://cestframework.com/">Cest Framework</a></li>
            <li><a href="https://ffmpeg.org/">ffmpeg</a></li>
            <li><a href="https://imagemagick.org/script/magick-wand.php">ImageMagick's MagickWand</a></li>
            <li><a href="https://imgur.com/">Imgur</a></li>
          </ul>
        </li>
      </ul>
      <p>
        Let's dive a bit deeper into the selection of these tools.
      </p>
      <h2>Rendering Backend</h2>
      <p>
        I wanted to use something easy to setup, open source to avoid any issues getting off-screen rendering to work, and flexible
        enough that would run in CI runners without too much hassle.
      </p>
      <p>
        Raysan's <a href="https://www.raylib.com/">Raylib</a> is a fantastic open source, super easy to use graphics library. It supports
        both 2D and 3D rendering, input, shaders... With a very clean C API and <a href="https://www.opengl.org/">OpenGL</a> backend.
      </p>
      <p>
        I was able to create a simple game and scenes with shaders to evaluate snapshot testing with it in a breeze. It doesn't support off-screen
        rendering out-of-the-box, but adding support was <a href="https://github.com/cegonse/offscreen-rendering-tests/blob/main/raylib-a5639bb-offscreen.patch">very straightforward</a>.
      </p>
      <p>
        Raylib supports using <a href="https://www.glfw.org/">GLFW</a> or <a href="https://www.libsdl.org/">SDL</a> as platform management backend libraries.
        GLFW comes with off-screen rendering support with <a href="https://gallium.readthedocs.io/en/latest/osmesa.html">libosmesa</a>, and with some patching I got it working with Raylib.
      </p>
      <p>
        <a href="https://gallium.readthedocs.io/en/latest/osmesa.html">libosmesa</a> is a C API to Gallium's software 3D renderer. A software
        implementation of the OpenGL spec, enabling applications to run graphics without a 3D accelerator. Neat!
      </p>
      <h2>Testing Environment</h2>
      <p>
        As with the rendering backend, I wanted to use tools that were easy to integrate, modify if required, and as hassle-free as possible so I could
        focus on the important bits.
      </p>
      <p>
        Using <a href="https://cestframework.com/">Cest Framework</a> for testing was a no-brainer, as it's my testing framework and I have full flexibility
        to adapt it to my needs.
      </p>
      <p>
        <a href="https://ffmpeg.org/">ffmpeg</a> and <a href="https://imagemagick.org/script/magick-wand.php">ImageMagick's MagickWand</a> were useful choices to
        generate snapshot test outputs, both in video and picture formats.
      </p>
      <p>
        Their APIs allow you to stitch pictures, create videos and many other changes programatically, which would be required for the expected-actual comparisons.
      </p>
      <p>
        And finally, <a href="https://imgur.com/">Imgur</a> is a convenient service to store pictures and videos with a simple public API. And free for public uploads!
      </p>
    </article>

    <article>
      <h2>Shader snapshot tests</h2>
      <div class="info">ℹ️ Find shader testing code at <a href="https://github.com/cegonse/offscreen-rendering-tests/tree/main/testing-shaders">testing-shaders</a></div>
      <p>
        Shaders are difficult to build and get right. Depending on the models and the desired effects, it can be hard
        to combine them and get everything right.
      </p>
      <p>
        Also, many times the artistic intent we want to convey through shaders is achieved by combining many shaders,
        applying them to different meshes, full-screen shaders and others.
      </p>
      <p>
        There are options to unit test shaders like <a href="https://github.com/cezbloch/shaderator">Shaderator by Cezary Bloch</a>.
        Tools like this are perfect to iteratively build complex kernels like compute shaders or complex visual shaders,
        but it might not be the best fit when the important outcome is the visual cohesion of the scene.
      </p>
      <p>
        Also, many times shaders are often built in visual scripting and node-based tools, which in turn generate
        source code that can be used by game engines. In these cases, unit testing wouldn't be easy or maybe even possible at all.
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/godot-shader-nodes.png" />
        <span>When working with node-based shader editors, code-based unit testing is out of the question. (<a href="https://stackoverflow.com/questions/65799797/godot-visual-shader-copy-code-from-fragment-to-vertex">Source</a>)</span>
      </div>
      <p>
        Snapshot testing looks like a great fit for this scenario. Let's lay out how could it be carried out.
      </p>
      <p>
        Let's try to replicate what would be done when building a complete scene with different meshes, particle effects,
        full screen shaders to do tone-mapping, lens correction, and so on.
      </p>
      <p>
        In this case, technical artists would tweak all the parameters as desired until getting it right.
      </p>
      <p>
        Storing a snapshot of the scene and testing it in the CI would help ensure the scene hasn't been unwillingly changed,
        for example when re-using the same shaders in another scene and tweaking them for it.
      </p>
      <p>
        I will keep the focus on the test code. Shaders themselves are example shaders included with Raylib, <a href="https://github.com/raysan5/raylib/blob/master/examples/shaders/resources/shaders/glsl330/bloom.fs">bloom</a> and <a href="https://github.com/raysan5/raylib/blob/master/examples/shaders/resources/shaders/glsl330/grayscale.fs">grayscale</a>
        full screen shaders.
      </p>
      <div class="fit-20">
        <pre><code>#include &lt;cest&gt;
#include &lt;render.h&gt;
#include &lt;verify.h&gt;

describe("Post-processing Camera Shaders", []() {
  it("renders all pixels with bloom effect", []() {
    RenderWithShader("common/bloom.fs");
    Verify();
  });

  it("renders all pixels in B&W", []() {
    RenderWithShader("common/grayscale.fs");
    Verify();
  });

  afterEach([]() {
    CleanUp();
  });
});
        </code></pre>
      </div>
      <p>
        These tests are very simple, and in fact, they could be simplified even further by parametrizing the shader location. The interesting parts are:
      </p>
      <ul>
        <li><b>Rendering</b> → Renders the scene with a sample mesh and applies the full-screen shader passed as an argument.</li>
        <li><b>Verifying</b> → Compares the rendered image with the stored snapshot, and throws an assertion error when the images are different, causing the test to fail.</li>
      </ul>
      <p>
        The interesting part lies in the <b>verification</b> part of the test. The steps it takes to check the currently rendered image against
        the snapshot are, roughly (if interested, check the details on GitHub)
      </p>
      <ul>
        <li>
          Take a screenshot of the current image.
          <ul><li>This will be the generated image to compare against the snapshot.</li></ul>
        </li>
        <li>
          If a snapshot image does not exist, create one.
          <ul>
            <li>This will only be the case the <b>first time</b> the test is run.</li>
            <li>In this first execution, the snapshot would be committed as the reference for the next runs.</li>
          </ul>
        </li>
        <li>Compare the current image to the generated image.
          <ul><li>ImageMagick's <a href="https://imagemagick.org/script/magick-wand.php">MagickWand API</a> was used to calculate the
            difference between both images.</li></ul>
        </li>
        <li>If the current image is different from the snapshot → <b>fail the test</b>.</li>
        <li>
          For convenience, store the test results so it can be reviewed later.
          <ul><li>Imgur API can be used to upload images and videos very easily (if privacy isn't a concern!).</li></ul>
        </li>
      </ul>
      <p>
        Take this example where the grayscale shader has been purposely modified to look like an unintended change.
      </p>
      <p>
        Instead of rendering a black and white image, it tints the result with a blue hue. As expected, the test fails since the
        rendering differs from the snapshot, and we get back the link to the result
        in the CI test execution:
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/shader-output.png" />
      </div>
      <p>
        And when checking the results, both the expected result and the actual result can be compared side by side, giving quick feedback on what has gone wrong.
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/shader-image.png" />
        <span>Left, expected result. Right, received result.</span>
      </div>
      <p>
        Without applying snapshot testing, the most probable way this unintended change would have been discovered when trying out the game.
      </p>
      <p>
        Perhaps by sheer coincidence, when another feature was being tested out and someone noticed the change and raised the bug.
      </p>
      <p>
        Depending on the nature of the change, it might end up completely unnoticed, and released in the final product.
      </p>
      <h2>Disclaimer on shader snapshot testing</h2>
      <p>
        Would this approach work on <b>every</b> type of scene and shader? Probably not.
      </p>
      <p>
        The fact that this example scene is extremely simple and does not take into account variability in the rendered image, such as shaders changing
        with time or other inputs cannot be overlooked.
      </p>
      <p>
        It assumes rendering is deterministic, otherwise every render would be slightly different when compared to the snapshot, making the comparison
        impossible.
      </p>
      <p>
        In any case, I believe this approach can be very valuable as a way to avoid changing how shaders look and feel by mistake.
      </p>
      <p>
        Maybe testing full scenes is not feasible as complexity increases, but this step already feels like a big improvement over having to manually noticed
        changes.
      </p>
    </article>

    <article>
      <h2>Game integration tests</h2>
      <div class="info">ℹ️ Find the relevant code at <a href="https://github.com/cegonse/offscreen-rendering-tests/tree/main/integration-testing">integration-testing</a></div>
      <p>
        We have seen snapshot testing shaders works and could be a good way to prevent accidental changes. How
        else could snapshot testing be applied to games?
      </p>
      <p>
        Another idea I had in mind is recording footage of game integration test, and comparing the results
        with a stored snapshot.
      </p>
      <p>
        When the integration test fails, the CI pipeline would report a video of the expected footage (the
        snapshot), along with what really happened.
      </p>
      <p>
        To put this concept to the test, I have built a very simple Flappy Bird clone:
      </p>
      <ul>
        <li>The pink square continuously advances to the right, scoring a point each time a column is traversed.</li>
        <li>The square falls by gravity, and jumps back by pressing the space key.</li>
        <li>If a column is touched, or the player goes out of the canvas, the player loses.</li>
      </ul>
      <p>
        The game is as simple as it gets, but it has what is needed to test it: a starting condition and a goal.
      </p>
      <div class="image fit-20">
        <video src="./images/offscreen-article/video_game.mp4" autoplay loop controls></video>
        <span>Simple enough, but it will let us put the concepts to test</span>
      </div>
      <p>
        In this snapshot test, the player will be set at the start and have zero points, and must have reached
        a goal after a while (scored several points).
      </p>
      <h2>Making the game testable</h2>
      <p>
        To be able to test the game in an integration test, there are some design decisions
        that must be taken beforehand.
      </p>
      <p>
        Parts of the game that prevent the game from being testable are put behind an interface, with
        the real implementation, and a mock that is only used by tests, handling:
      </p>
      <ul>
        <li><b>Input</b> → The only way to win the game is by pressing the space bar, but of course
        this isn't possible from an automated test.
        <ul>
          <li>This can be fixed by simulating inputs when needed, so the player will automatically
            jump when needed.
          </li>
          <li>Again, this works thanks to the game being extremely simple.</li>
        </ul>
        </li>
        <li><b>Randomness</b> → The game generates obstacles at random heights, making the result
        impossible to predict against a stored snapshot.
        <ul>
          <li>To get around this, the game under test has a controlled random number generation.</li>
          <li>This ensures the randomly generated map stays the same across executions.</li>
        </ul>
        </li>
      </ul>
      <p>
        These, among many other design decisions, must be taken beforehand to ensure the game is testable.
        Think about it like trying to add online play mid-development.
      </p>
      <p>
        As with any other complex software project, making it testable when it has not been designed towards
        testability will be a nightmare.
      </p>
      <p>
        Most times, we will be dealing with a much more complex game. An approach I have tried out for
        these cases is building an AI that is capable of achieving the game's main goals without supervision.
      </p>
      <p>
        Then, the integration test can be run against the AI and if something is changed by mistake,
        the test fails.
      </p>
      <p>
        That is out of scope for this article though, but I might write about it in detail in the future.
      </p>
      <h2>The integration test</h2>
      <p>
        Now that the game is testable, let's take a look at the actual integration test code:
      </p>
      <div class="fit-20">
        <pre><code>#include &lt;cest&gt;
#include &lt;game.h&gt;
#include &lt;platform-test.h&gt;
#include &lt;test-helper.h&gt;

describe("JumpingGame", []() {
  it("builds up to 2 points after traversing several columns", []() {
    auto headless_mode = true;
    Game game(headless_mode);

    onEveryNthFrame(6, [](int _) { Platform::ForceJumpKey(); });
    onEveryNthFrame(HEADLESS_MODE_FRAMESKIP, [](int frame) { Screenshot(frame); });
    runFrames(NUM_FRAMES_TO_RENDER, [&]() { game.DoFrame(); });

    auto assertion = [&]() { expect(game.Score()).toBeGreaterThan(2); };
    Verify(assertion);
  });
});
        </code></pre>
      </div>
      <p>
        In this case, the integration test works as a harness to run the game loop.
        Dividing the test into three core sections, it consists of:
      </p>
      <ul>
        <li><b>Arrangement</b> → Set up the game and configure it to run in headless mode.
          <ul><li>Thanks to using the patched Raylib version, the game runs with full graphics
          without needing GPU or a windowing environment.</li></ul>
        </li>
        <li><b>Action</b> → Run the game for several frames, and perform actions when required.
          <ul><li>In this case, every 6 frames the mocked spacebar is pressed to jump, and a screenshot is taken every 10th frame.</li></ul>
        </li>
        <li><b>Assertion</b> → After running the game, verify that it did as expected.
          <ul><li>Given a designated number of frames, the score must be at least 2 points.</li></ul>
        </li>
      </ul>
      <p>
        The test harness stores the callbacks passed to <span class="inline-code">onEveryNthFrame()</span>,
        and executes them when <span class="inline-code">runFrames()</span> reaches the expected frame.
      </p>
      <p>
        The <span class="inline-code">Verify()</span> function forces the test to fail when the passed lambda expression fails to evaluate,
        records a new video combining the stored snapshot with the received result, and uploads it to Imgur
        to allow checking it out if the CI fails.
      </p>
      <p>
        Let's try simulating an example where game logic might get mistakenly broken, and force a CI failure. In this
        example, the jumping code has been commented out, thus disabling the jumping mechanic.
      </p>
      <p>
        In this case, the score will never become greater than two, as you must jump to pass the first
        column, and thus increase the score.
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/game-test-output.png" />
        <span>CI output, with the link to the video recording of the test failure.</span>
      </div>
      <p>
        When opening the video, the results can be checked side-by-side. While the stored snapshot manages
        to get to the third column, the current code cannot get past the first one.
      </p>
      <div class="image fit-20">
        <video src="./images/offscreen-article/video_1.mp4" autoplay loop controls></video>
        <span>Left, expected result. Right, received result.</span>
      </div>
      <h2>Performance problems with software rendering</h2>
      <p>
        In the previous video, you will have noticed that the footage looks weird. It looks like
        the player is floating around, nothing like what can be seen in the first video.
      </p>
      <p>
        The truth is software rendering is <b>very slow</b>, even for such a simple scene as this
        one. I expected it to perform badly, but not <i>this badly</i>.
      </p>
      <p>
        The test snapshots are recorded at 1/10th of the real framerate, as otherwise the rendering
        code won't be able to keep up. It barely gets above 15 FPS on an AMD 3700X.
      </p>
      <p>
        Also, framerate varies greatly depending on raw CPU power. Shared GitHub Actions runners are much,
        much slower than what you get from your development machine.
      </p>
      <p>
        This makes me think software rendering is not a good option for this kind of tests. Resorting to
        a discrete GPU in a dedicated runner will allow for good quality footage of test failures.
      </p>
      <p>
        When something fails, it's best to get good quality feedback, so it can be fixed as quickly as
        possible.
      </p>
    </article>

    <article>
      <h2>Closing thoughts</h2>
      <p>
        After going through these two concepts and playing around with the possibilities, it seems like:
      </p>
      <p>
        ✅ Snapshot testing shaders works very well, and should be scalable to work with complex shaders.
      </p>
      <ul>
        <li>Breaking shaders is easy, and gets worse as the complexity of the code base grows.</li>
        <li>Testing shaders in a shader editor isn't always enough, as there could game specific settings
          besides the shader code itself.
        </li>
        <li>Resorting to doing a playthrough to do a final check shouldn't be the only way to verify their correctness.</li>
        <li>It is a cost-effective way to avoid artistic regressions when modifying shaders.</li>
      </ul>
      <p>
        ⚠️ However, some changes might make getting this to work in a real scenario hard.
      </p>
      <ul>
        <li>
          Getting these techniques to work with commercial engines might get cumbersome.
          <ul>
            <li>For example, getting these techniques to work with Unity or Unreal might require a lot of tweaking.</li>
          </ul>
        </li>
        <li>
          Unless the game runs on OpenGL, there is no way to use libosmesa to do software rendering.
          <ul>
            <li>There are software rendering alternatives for Vulkan, but...</li>
            <li>In most cases, using a discrete GPU probably will be simpler.</li>
          </ul>
        </li>
      </ul>
      <p>
        ❌ And finally, regarding integration testing, using libosmesa to avoid GPU rendering won't work in most cases.
      </p>
      <ul>
        <li>Software offscreen rendering with libosmesa is <b>very</b> slow</li>
          <ul>
            <li>Shader testing is doable, but rendering full animated scenes is out of the question.</li>
          </ul>
        </li>
        <li>Any engines or games using other rendering backends different from OpenGL won't work with libosmesa.</li>
        <li>
          It makes much more sense to run game integration tests with a real GPU.
          <ul>
            <li>There is no other way to get quality footage of the results.</li>
            <li>CI runners will need a dedicated GPU and specific configuration.</li>
          </ul>
        </li>
      </ul>
    </article>
    <hr />
    <article>
      <p>
        If you've made it this far, thank you <b>so much</b> for reading!
      </p>
      <p>
        If you have any comments, suggestions or want to discuss any of the topics in this article,
        feel free to reach out through any of the channels in the top of the page
        or through the <a>LinkedIn</a> post!
      </p>
    </article>

    <footer><a href="./index.html">Home</a></footer>

    <script>
    hljs.highlightAll();

    const resizeIframes = () => {
      const isPortrait = window.matchMedia("(orientation: portrait)").matches;
      document.querySelectorAll("iframe").forEach((node) => {
        const contentWidth = document.querySelector("header").scrollWidth;
        const targetWidth = isPortrait ?
          contentWidth :
          contentWidth * 0.5;
        const targetHeight = targetWidth * 0.7;

        node.width = targetWidth;
        node.height = targetHeight;
      });
    };

    const makeResourcesClickable = () => {
      document.querySelectorAll("img").forEach((node) => {
        node.addEventListener("click", () => {
          window.open(node.src);
        });
      });
    };

    window.addEventListener("resize", resizeIframes);
    resizeIframes();
    makeResourcesClickable();
    </script>
  </body>
</html>
