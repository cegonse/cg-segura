<html lang="en">
  <head>
    <meta property="og:title" content="César González Segura - Applying snapshot testing to game development and 3D graphics">
    <meta property="og:description" content="Explore how to combine off-screen rendering and software rendering to build snapshot tests for complex 3D visualizations">
    <meta property="og:image" content="https://cesargonzalez.dev/photo.webp">
    <meta property="og:url" content="https://cesargonzalez.dev">
    <meta name="description" content="César González Segura - Applying snapshot testing to game development and 3D graphics">
    <meta charset="utf-8"/>
    <title>César González</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/default.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/atom-one-dark.min.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>
      html {
        color: rgb(236, 233, 230);
        background-color: #1b2733;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        width: 100%;
      }

      @media (orientation: landscape) {
        body {
          margin-left: 20%;
          margin-right: 20%;
        }
      }

      @media (orientation: portrait) {
        body {
          margin-left: 5%;
          margin-right: 5%;
        }
      }

      header {
        display: flex;
        flex-direction: row;
        align-items: center;
      }

      @media (orientation: landscape) {
        header img {
          height: 2em;
          width: 2em;
          border-radius: 50%;
          margin-right: 1em;
        }
      }

      @media (orientation: portrait) {
        header {
          display: flex;
          flex-direction: column;
          align-items: center;
        }

        header img {
          height: 5em;
          width: 5em;
          border-radius: 50%;
        }
      }

      h2 {
        font-size: 2.5em;
        margin-bottom: 0.1em;
      }

      h3 {
        font-size: 1.5em;
        margin-bottom: 0;
      }

      p {
        text-align: justify;
        font-size: 1.2em;
      }

      li {
        font-size: 1.2rem;
        margin-top: 0.5rem;
        margin-bottom: 0.5rem;
      }

      a {
        color: #758CFF;
        text-decoration: underline;
        padding-bottom: 0;
      }

      a:hover {
        color: #4D6AFF;
        text-decoration: underline;
      }

      .image {
        display: flex;
        flex-direction: column;
        align-items: center;

        img {
          border-radius: 8px;
          object-fit: contain;
          width: 100%;
          height: auto;
          margin-top: 1em;
        }

        video {
          border-radius: 8px;
          object-fit: contain;
          width: 100%;
          height: auto;
          margin-top: 1em;
        }

        span {
          margin-top: 1em;
          margin-bottom: 0.5em;
          font-style: italic;
        }
      }

      @media (orientation: landscape) {
        .fit-20 {
          margin-left: 20%;
          margin-right: 20%;
        }
      }

      article {
        margin-top: 4rem;
        margin-bottom: 4rem;
      }

      article > p:first-child {
        margin-top: 2em;
      }

      article > a {
        display: inline-block;
      }

      article > h2 {
        font-size: 2em;
      }

      pre > code {
        border-radius: 8px;
      }

      pre {
        width: 100%;
      }

      iframe + span {
        margin-top: 0.5em;
        margin-bottom: 0.5em;
        font-style: italic;
        text-align: center;
      }

      video {
        border-radius: 8px;
      }

      video + span {
        margin-top: 0.5em;
        margin-bottom: 0.5em;
        font-style: italic;
        text-align: center;
      }

      pre + span {
        margin-bottom: 0.5em;
        font-style: italic;
        font-family: system-ui, -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
        text-align: center;
      }

      @media (orientation: landscape) {
        article > pre {
          margin-left: 10%;
          margin-right: 10%;
          margin-top: 2em;
          margin-bottom: 2em;
        }
      }
      @media (orientation: portrait) {
        article > pre {
          margin-top: 2em;
          margin-bottom: 2em;
        }
      }

      article > div {
        display: flex;
        flex-direction: column;
        align-items: center;
      }

      article > div > table {
        text-align: center;
        border-collapse: separate;
        border-spacing: 2em 0;
        font-size: 1.2em;
      }

      table + span {
        font-style: italic;
        margin-top: 1em;
        margin-bottom: 1em;
        text-align: center;
      }

      .italic {
        font-style: italic;
      }

      .contact {
        display: flex;
        flex-direction: row;
      }

      .contact img {
        height: 2em;
        margin-right: 1em;
      }

      .contact a:hover {
        transform: scale(1.05);
        filter: brightness(0.8)
      }

      @media (orientation: portrait) {
        .contact {
          justify-content: space-around;
        }

        .contact img {
          height: 48px;
        }
      }

      footer {
        width: 100%;
        height: 3em;
        display: flex;
        flex-direction: row;
        justify-content: center;
        align-items: center;
        background-color: #263748;
        border-radius: 8px;
        margin-top: 4em;
        margin-bottom: 1em;
      }

      @media (orientation: portrait) {
        footer {
          font-size: 1.5em;
          border-radius: 32px;
        }
      }

      .info {
        background-color: #42586e;
        border-radius: 8px;
        padding: 1em;
        display: block !important;
        margin-top: 1rem;
        margin-bottom: 1rem;
      }
    </style>
  </head>
  <body>
    <header>
      <img src="images/photo.webp" alt="My Picture" />
      <h1>César González Segura</h1>
    </header>

    <div class="contact">
      <a aria-label="Telegram Contact" href="https://t.me/cegonse"><img src="images/telegram.webp" alt="Telegram Logo"></a>
      <a aria-label="LinkedIn Profile" href="https://www.linkedin.com/in/cesar-gonzalez-segura/"><img src="images/linkedin.webp" alt="LinkedIn Logo"></a>
      <a aria-label="Email Contact" href="mailto:contact@cesargonzalez.dev"><img src="images/email.webp" alt="Email Symbol"></a>
      <a aria-label="GitHub Profile" href="https://github.com/cegonse"><img src="images/github.webp" alt="GitHub Logo"></a>
    </div>

    <h2>Applying snapshot testing to game development</h2>
    <p class="italic">Posted July _th 2025</p>
    <p class="italic">Did you enjoy this article? Follow the conversation on <a>LinkedIn</a></p>
    <article>
      <div class="info">ℹ️ All code used to prepare this article is available in GitHub at <a href="https://github.com/cegonse/offscreen-rendering-tests">cegonse/offscreen-rendering-tests</a></div>
      <p>
        Getting a game to feel, look and play as intended is hard. There's a delicate balance that must be taken care of at all times.
        Any seemingly tiny change could have enormous consequences on how a game looks and feels.
      </p>
      <p>
        Applying testing techniques to games as if they were any software product does not always cut it.There are nuances in how a game
        <i>feels</i> that are hard to capture with unit tests.
      </p>
      <p>
        Systems are so complex and interconnected that it becomes really hard to keep track of how changes to shared modules might
        affect the whole game.
      </p>
      <p>
        Manual testing the whole game everytime a change is done to core mechanics or graphics isn't feasible, except for very small
        games.
      </p>
      <p>
        Those manual tests could be automated too, but as with all heavy weight full-app tests, building and maintaining them is time consuming
        and prone to breaking.
      </p>
      <p>
        I wanted to explore a lightweight way to test full sub-sets of the game, without having to rely on playing the <i>actual</i> game.
      </p>
      <p>
        Can we run small segments of the game and compare the results with stored <i>snapshots</i>, and find out if anything relevant has
        changed?
      </p>
      <p>
        In this article, I've explored how to apply <b>snapshot testing</b> to reduce unintended changes when building games.
      </p>
      <p>
        There are many scenarios where snapshot testing can be quite useful, giving us a break on writing heavy acceptance tests. But it also comes
        with a number of caveats, so it won't be applicable in all cases.
      </p>
    </article>
    <article>
      <h2>Testing and game development</h2>
      <p>
        Since I first started learning about software automated testing, I began thinking about how could it be
        effectively applied to game development.
      </p>
      <p>
        If you have ever developed a videogame, you will have experienced first-hand how complex game development can be.
        Coindidentally, the environment where I've seen less automated testing through my career is game development.
      </p>
      <p>
        I've mostly worked on very small games, with one exception being what the industry calls
        <a href="https://en.wikipedia.org/wiki/AAA_(video_game_industry)#III"><i>Triple-I</i></a> games,
        so probably I've missed out on how big studios handle testing.
      </p>
      <p>
        Reading from sites like <a href="https://www.gamedeveloper.com/">GameDeveloper</a> and remembering conversations with
        people working on established studios, I learned that many rely on automated QA tests based on macros.
      </p>
      <p>
        These tests auto-play portions of the game from recorded controller inputs and bespoke markers detect whether a goal has
        been reached or not. This requires launching the full game, and as with all QA acceptance tests, are
      </p>
      <ul>
        <li>Expensive to implement.</li>
        <li>Prone to breaking upon change.</li>
        <li>Expensive to maintain to keep them working.</li>
        <li>Require powerful hardware to run (as powerful as the hardware required to run the game itself).</li>
      </ul>
      <p>
        QA acceptance tests are of course necessary and must always be present in any product's testing pyramid. But,
        how could we reduce the cost (both building, maintaining and running) of all our test suite?
      </p>
      <p>
        A friend of mine introduced me to how <a href="https://store.steampowered.com/app/427520/Factorio">Factorio</a> tests its vast number of mechanics, and it rang a bell.
      </p>
      <div>
        <iframe width="auto" height="auto" src="https://www.youtube.com/embed/CgMV2dFFdFE" title="Factorio: Space Age - Graphics mode tests" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        <span><a href="https://store.steampowered.com/app/427520/Factorio">Factorio's</a> testing mode resembles what I wanted to achieve</span>
      </div>
      <p>
        What about running smaller integration tests of only a subset of the full game? And all this while avoiding running them on retail hardware?
        For example, directly in GitHub Actions CI runners.
      </p>
      <p>
        The idea I came up with was combining a couple technical concepts to achieve this, <b>snapshot testing</b> and <b>off-screen rendering</b>.
      </p>
    </article>

    <article>
      <h2>Understanding the concepts</h2>
      <p>Asd</p>
    </article>

    <article>
      <h2>Shader and scene snapshot tests</h2>
      <div class="info">ℹ️ Find shader testing code at <a href="https://github.com/cegonse/offscreen-rendering-tests/tree/main/testing-shaders">testing-shaders</a></div>
      <p>
        Shaders are difficult to build and get right. Depending on the models and the desired effects, it can be hard
        to combine them and get everything right.
      </p>
      <p>
        Also, many times the artistic intent we want to convey through shaders is achieved by combining many shaders,
        applying them to different meshes, full-screen shaders and others.
      </p>
      <p>
        There are options to unit test shaders like <a href="https://github.com/cezbloch/shaderator">Shaderator by Cezary Bloch</a>.
        Tools like this are perfect to iteratively build complex kernels like compute shaders or complex visual shaders,
        but it might not be the best fit when the important outcome is the visual cohesion of the scene.
      </p>
      <p>
        Also, many times shaders might are built in visual scripting and node-based tools, which in turn generate
        source code that can be used by game engines. In these cases, unit testing wouldn't be easy or maybe possible at all.
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/godot-shader-nodes.png" />
        <span>When working with node based shader editors, code-based unit testing is out of the question. (<a href="https://stackoverflow.com/questions/65799797/godot-visual-shader-copy-code-from-fragment-to-vertex">Source</a>)</span>
      </div>
      <p>
        Snapshot testing looks like a great fit for this scenario. Let's lay out how could it be carried out.
      </p>
      <p>
        Let's try to replicate what would be done when building a complete scene with different meshes, particle effects,
        full screen shaders to do tone-mapping, lens correction, and so on.
      </p>
      <p>
        In this case, technical artists would tweak all the parameters as desired until getting it right.
      </p>
      <p>
        Storing a snapshot of the scene and testing it in the CI would help ensure the scene hasn't been unwillingly changed,
        for example when re-using the same shaders in another scene and tweaking them for it.
      </p>
      <p>
        I will keep the focus on the test code. Shaders themselves are example shaders available from Raylib, <a href="https://github.com/raysan5/raylib/blob/master/examples/shaders/resources/shaders/glsl330/bloom.fs">bloom</a> and <a href="https://github.com/raysan5/raylib/blob/master/examples/shaders/resources/shaders/glsl330/grayscale.fs">grayscale</a>
        full screen shaders.
      </p>
      <div>
        <pre><code>#include &lt;cest&gt;
#include &lt;render.h&gt;
#include &lt;verify.h&gt;

describe("Post-processing Camera Shaders", []() {
  it("renders all pixels with bloom effect", []() {
    RenderWithShader("common/bloom.fs");
    Verify();
  });

  it("renders all pixels in B&W", []() {
    RenderWithShader("common/grayscale.fs");
    Verify();
  });

  afterEach([]() {
    CleanUp();
  });
});
        </code></pre>
      </div>
      <p>
        These tests are very simple, and in fact, it could be simplified even further by parametrizing the shader location. The interesting parts are:
      </p>
      <ul>
        <li><b>Rendering</b> → Renders the scene with a sample mesh and applying the full-screen shader passed as an argument.</li>
        <li><b>Verifying</b> → Compares the rendered image with the stored snapshot, and throws an assertion error when the images are different, causing the test to fail.</li>
      </ul>
      <p>
        The interesting part lies in the <b>verification</b> part of the test. The steps it takes to check the currently rendered image against
        the snapshot are, roughly (if interested, check the details in GitHub)
      </p>
      <ul>
        <li>
          Take a screenshot of the current image.
          <ul><li>This will be the generated image to compare against the snapshot.</li></ul>
        </li>
        <li>
          If an snapshot image does not exist, create one.
          <ul>
            <li>This will only be the case the <b>first time</b> the test is run.</li>
            <li>In this first execution, the snapshot would be committed as the reference for next runs.</li>
          </ul>
        </li>
        <li>Compare the current image to the generated image.
          <ul><li>ImageMagick's <a href="https://imagemagick.org/script/magick-wand.php">MagickWand API</a> was used to calculate th
            difference between both images.</li></ul>
        </li>
        <li>If the current image is different from the snapshot → <b>fail the test</b>.</li>
        <li>
          For convenience, store the test results so it can be reviewed later.
          <ul><li>Imgur API can be used to upload images and videos very easily (if privacy isn't a concern!).</li></ul>
        </li>
      </ul>
      <p>
        Take this example where the grayscale shader has been purposely modified to look like an unintended change.
      </p>
      <p>
        Instead of rendering a black and white image, it tints the result with a blue hue. As expected, the test fails since the
        rendering differs from the snapshot, and we get back the link to the result
        in the CI test execution:
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/shader-output.png" />
      </div>
      <p>
        And when checking the results, both the expected result and the actual result can be compared side by side, giving quick feedback of what has gone wrong.
      </p>
      <div class="image fit-20">
        <img src="./images/offscreen-article/shader-image.png" />
        <span>Left, expected result. Right, received result.</span>
      </div>
      <p>
        Without applying snapshot testing, the most probable way this unintended change would have been discovered would have been when trying out the game.
      </p>
      <p>
        Perhaps by sheer coincidence, when another feature was being tested out and someone noticed the change and raised the bug.
      </p>
      <p>
        Depending on the nature of the change, it might would not have been ever noticed, and released in the final product.
      </p>
      <h2>Disclaimer on shader snapshot testing</h2>
      <p>
        Would this approach work on <b>every</b> type of scene and shader? Probably not.
      </p>
      <p>
        The fact that this example scene is extremely simple and does not take into account variability in the rendered image, such as shaders changing
        with time or other inputs cannot be overlooked.
      </p>
      <p>
        It assumes rendering is deterministic, otherwise every render would be slightly different when compared to the snapshot, making the comparison
        impossible.
      </p>
      <p>
        In any case, I believe this approach can be very valuable as a way to avoid changing how shaders look and feel by mistake.
      </p>
      <p>
        Maybe testing full scenes is not feasible as complexity increases, but this step already feels like a big improvement over having to manually noticed
        changes.
      </p>
    </article>

    <article>
      <h2>Game integration tests</h2>
      <div class="info">ℹ️ Find the relevant code at <a href="https://github.com/cegonse/offscreen-rendering-tests/tree/main/integration-testing">integration-testing</a></div>
      <p>
        Asd
      </p>
      <div class="image fit-20">
        <video src="./images/offscreen-article/video_game.mp4" autoplay loop controls></video>
        <span>Simple enough but it will let us put the concepts to test</span>
      </div>
      <p>
        Asd
      </p>
      <div>
        <pre><code>#include &lt;cest&gt;
#include &lt;game.h&gt;
#include &lt;platform-test.h&gt;
#include &lt;test-helper.h&gt;

describe("JumpingGame", []() {
  it("builds up to 2 points after traversing several columns", []() {
    auto headless_mode = true;
    Game game(headless_mode);

    onEveryNthFrame(6, [](int _) { Platform::ForceJumpKey(); });
    onEveryNthFrame(HEADLESS_MODE_FRAMESKIP, [](int frame) { Screenshot(frame); });
    runFrames(NUM_FRAMES_TO_RENDER, [&]() { game.DoFrame(); });

    auto assertion = [&]() { expect(game.Score()).toBeGreaterThan(2); };
    Verify(assertion);
  });
});
        </code></pre>
        <span>We'll review the parts of the game's acceptance test next</span>
      </div>
      <p>
        Asd asd asd asd asd asd
      </p>
      <div class="image fit-20">
        <video src="./images/offscreen-article/video_1.mp4" autoplay loop controls></video>
        <span>The end result pulled right from the CI pipeline</span>
      </div>
      <p>Asd asd asd asd as dasd</p>
    </article>

    <article>
      <h2>Closing thoughts</h2>
      <p>
        Asd
      </p>
      <p>
        What is it useful for
      </p>
      <ul>
        <li>Asd</li>
        <li>Asd</li>
        <li>Asd</li>
      </ul>
      <p>What are the main challenges</p>
      <ul>
        <li>Asd</li>
        <li>Asd</li>
        <li>Asd</li>
      </ul>
      <p>
        What is not going to work
      </p>
      <ul>
        <li>Asd</li>
        <li>Asd</li>
        <li>Asd</li>
      </ul>
    </article>

    <footer><a href="./index.html">Home</a></footer>

    <script>
    hljs.highlightAll();

    const resizeIframes = () => {
      const isPortrait = window.matchMedia("(orientation: portrait)").matches;
      document.querySelectorAll("iframe").forEach((node) => {
        const contentWidth = document.querySelector("header").scrollWidth;
        const targetWidth = isPortrait ?
          contentWidth :
          contentWidth * 0.5;
        const targetHeight = targetWidth * 0.7;

        node.width = targetWidth;
        node.height = targetHeight;
      });
    };

    window.addEventListener("resize", resizeIframes);
    resizeIframes();
    </script>
  </body>
</html>
